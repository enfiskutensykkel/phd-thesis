\chapter{Conclusion}\label{chapter:conclusion}
As distributed and parallel computing applications are becoming increasingly compute-intensive and data-driven, \gls{io} performance demands are ever growing.
%
Computing accelerators (such as \glspl{fpga} and \glspl{gpu}), high-throughput \glspl{nic}, and fast storage devices like \glspl{nvme}, are now commonplace in most modern computer systems.
%
Nevertheless, distributing such \gls{io} resources in a way that maximizes both performance and resource utilization is a challenge for heterogeneous computing clusters. 
%
To avoid that individual machines becoming performance bottlenecks, resources must be shared efficiently between machines in the cluster.



In this dissertation, we have addressed this challenge and presented our SmartIO framework for sharing \gls{io} resources between machines connected over \gls{pcie}.
%
Our SmartIO framework effectively makes all machines, including their internal devices and memory, part of a common \gls{pcie} domain.
%
The hard separation between local and remote is blurred out, as machines can dynamically share their internal devices and memory resources with other machines in a \gls{pcie} network. 
%
Remote resources can be used as if they were installed in the local machine, without any performance degradation compared to local access and without requiring adaptions to device drivers or application software.



\section{Summary}\label{sec:summary}
% - What was the target problem?
Connecting two or more computer systems over \gls{pcie} is possible by using \glspl{pcientb}.
%
\Glspl{ntb} have memory address translation capabilities that makes it possible for a machine to map \glspl{segment} of remote memory directly into local address space.
%
However, leveraging \glspl{ntb} to share the internal devices and memory of a machine with other, remote machines is a challenge, as the use of a remote resource requires software to be aware of the fact that the resource is on the other side of an \gls{ntb}.
%
For example, a device driver operating a remote device must use addresses that correspond to the remote device's address space when initiating \gls{dma} transfers or configuring interrupts.
%
This additional complexity makes it infeasible to rely on \glspl{ntb} alone to implement a resource sharing solution, as it would require extensive modifications to existing software.



% - What did you develop?
To solve this, we have developed our SmartIO framework for sharing devices and memory resources between machines connected with \glspl{ntb}.
%
Our solution consists of ``\glspl{lender}'', machines lending out one or more of its internal devices, and ``\glspl{borrower}'', machines using such a device.
%
Machines can act as \gls{lender} and \gls{borrower} at the same time, making SmartIO fully distributed.
%
Any type of \gls{pcie} device may be shared, as SmartIO is built on standard \gls{pcie}.
%
SmartIO keeps track of which machines devices and \glspl{memorysegment} reside in, and is able to map resources on behalf of devices and resolve memory addresses as they are seen by devices.
%
As such, SmartIO provides a logical decoupling of devices and which \glspl{lendermachine} they are installed in, solving the challenge of managing multiple address spaces and making remote resources appear and behave as if they are local.




SmartIO supports three different methods of device sharing:
%
\begin{itemize}
    \item Our \textbf{\gls{dl}} sharing method makes it possible to dynamically assign a \gls{pcie} device to a remote \gls{borrowermachine}.
        %
        By using a ``\gls{shadowdev}'', the device appears \gls{hotadded} to the local device tree on the \gls{borrower}.
        %
        The fact that the device is remote is made transparent to the system, allowing the device to be used by native device drivers and application software as if it was locally installed.


    \item Our \textbf{\gls{mdev}} extension to the \gls{kvm} makes it possible to distribute devices to \glspl{vm} running on remote machines, by facilitating \emph{\gls{passthrough}} of a device to the \gls{vmguest}.
        %
        Application software and device drivers running inside the \gls{vmguest} can directly interact the physical device, without compromising the isolation of the virtualized environment.


    \item Our \textbf{\gls{sisciapiext}} makes it possible to \gls{disaggregate} devices and memory resources in software.
        %
        We have extended the \gls{sisciapi} with device-oriented programming semantics and device driver support functionality, making core SmartIO capabilities available through the same shared-memory \gls{api} used to write cluster applications.
        %
        Using this \gls{apiext}, we have also implemented a \textbf{proof-of-concept \gls{nvme} driver} that demonstrates how devices can be \gls{disaggregated} and shared with multiple machines at the same time.
        
\end{itemize}




% - What were the results?
We have performed an extensive performance evaluation, consisting of a comprehensive collection of synthetic performance benchmarking and realistic workloads.
%
We have made a point out of using standard benchmarking software and device drivers, as well as a wide variety of \gls{pcie} devices, in order to demonstrate the completeness of our SmartIO framework.
%
Particularly, we have performed comparison tests where we compare the performance of a workload using remote resources to the same workload running only on a local system.
%
The results prove that, when conditions are similar, the SmartIO sharing methods \textbf{do not add \emph{any} performance overhead} compared to using local resources.
%
Furthermore, we have also explored how different network topologies affect the performance, and have identified situations where the \gls{iommu} can become a potential performance bottleneck.
%
Finally, our exhaustive performance test suite also includes tests using our proof-of-concept \gls{nvme} driver that highlights possibilities that are enabled by our shared-memory approach to device sharing.






\section{Revisiting the problem statement}\label{sec:discussion}
Although many \gls{disaggregation} solutions for sharing resources over a network already exist, these solutions are often inadequate as discussed in \cref{sec:rw}. 
%%
For instance, \gls{disaggregation} solutions based on \gls{rdma} introduce additional software complexity and indirections that leads to a disparity in performance, compared to a local machine using local resources. 
%%
\Gls{pcie}-based \gls{disaggregation} solutions do not have such performance issues, since they facilitate the use of resources using native \gls{pcie}.
%%
However, \gls{pcie}-based \gls{disaggregation} solutions are limited to sharing devices installed in dedicated servers, as they lack the shared-memory capabilities necessary for sharing the \emph{inner} resources of individual machines. 



Thus, the main goal of this dissertation was not simply to implement yet another \gls{disaggregation} solution, but developing a new, more flexible solution with zero overhead by taking a novel approach:
%
leveraging the memory mapping capabilities of \glspl{ntb} to unify traditional device \gls{io} with distributed, shared-memory computing.
%
Such a solution should allow the inner devices and memory of machines to be shared with, and used by, remote machines in a cluster, as if these resources were local to the remote machines using them.
%
In \cref{sec:problem}, we broke down the challenges of this goal into \crefrange{obj:distributed}{obj:experiments}:



% How did we answer objectives 1-6? 
% For each objective, state clearly how it was solved and the contribution of it (refer back to contributions in 1.5)
% Also make it clear how the contribution helps solve the overall research question as well.



\objdistributed*%
%
Using SmartIO, machines act as ``\glspl{lender}'' and ``\glspl{borrower}''. 
%
\Gls{lender} registers one or more of its devices with SmartIO, allowing these devices to be used by remote machines.
%
A \gls{borrower} is a system that is currently using such a device.
%
Any \gls{pcie} device may be registered with SmartIO and shared, as demonstrated particularly by our extensive performance evaluation in \paperref{tocs:eval}.
%
We implemented three different sharing methods for our solution, namely \gls{dl}, \gls{mdev}, and the \gls{exttosisciapi}.
%
These sharing methods are detailed in \cref{nossdav,srmpds,cc,tocs}.
%
SmartIO is fully distributed, allowing any machine to act as \gls{lender} or \gls{borrower}, or even acting as both at the same time.
%
As such, SmartIO enables a peer-to-peer sharing model, where all machines in the cluster can participate in the sharing through contributing their own resources and using resources shared by others.
%
This sets SmartIO apart from existing \gls{pcie}-based solutions (including Ladon~\cite{Tu2014}), as these are only able to share devices in dedicated servers.



\objtransparent*%
%
As mentioned above, our SmartIO framework includes three sharing methods:
%
\begin{itemize}
    \item The \gls{dl} sharing method inserts a remote device into the local device tree of the \gls{host}~\gls{os} by using a ``\gls{shadowdev}'', as described in \cref{sec:lending}.
        %
        This allows device drivers, application software, and even the \gls{os} itself to use the remote device through \emph{native} \gls{os} interfaces, in the same way they would use a local device.
        %
        The initial \gls{dl} is described in \cref{nossdav}. Subsequent improvements, such as supporting borrowing devices from several \glspl{lender}, are detailed in \cref{srmpds,cc,tocs}.
    


    \item The \gls{mdev} extension to the \gls{kvm} makes it possible to \gls{passthrough} remote devices to a \gls{vm}. 
        %
        Software running in the \gls{guest}, including device drivers and the \gls{guest}~\gls{os}, may interact with the physical devices directly, without escaping the virtualized environment.
        %
        To the \gls{vmguest}, the devices appear as locally installed devices.
        %
        The initial \gls{mdev} is described in \cref{srmpds}, and an improved version with a mechanism for discovering \gls{guestphys} memory layout is described in \cref{cc,tocs}.



    \item The \gls{apiext} brings device-oriented programming semantics and device driver support functions to the \gls{sisciapi}.
        %
        Using the \gls{apiext}, \gls{userspace} device drivers can be implemented using the same \gls{api} used to implement shared-memory communication using \glspl{ntb}.
        %
        As the \gls{apiext} is built on top of SmartIO, the physical location of devices and \glspl{memorysegment} is abstracted away.
        %
        \Gls{userspace} device driver using our extension can be written as if all resources are local.
        %
        The \gls{apiext} is described in \cref{tocs}.
\end{itemize}
%
These three sharing methods set SmartIO apart from \gls{rdma}-based \gls{disaggregation} solutions, as we do not require any \glspl{middlewareservice} and avoid any adaptations to existing software.
%
Scaling out becomes significantly easier, as SmartIO allows remote resources to be used natively instead.



\objperformance*%
%
Contrary to \gls{rdma}-based \gls{disaggregation} solutions, with SmartIO it is not necessary to interact with a device driver running on the remote system, as remote resources can be accessed over standard \gls{pcie}.
%
However, in order to avoid communication overhead in the performance-critical path, memory-maps must be prepared ahead of time.
%
To accomplish this, our \gls{dl} and \gls{mdev} sharing methods use the \gls{borrower}'s \gls{iommu} to create continuous memory ranges that can be mapped as ``\glspl{dmawindow}'' through the appropriate \glspl{ntb} in advance of use.
%
The \gls{dl} sharing method uses the \gls{shadowdev} to intercept when a device driver on the \gls{borrower} creates \gls{dma} buffers, and is able to dynamically add memory pages to the local \gls{iommu} ranges.
%
We describe this in \cref{srmpds,cc,tocs}.
%
The \gls{mdev} sharing method additionally uses the \gls{lender}'s \gls{iommu} to map the device to the same \gls{guestphys} memory layout as the \gls{vmguest}.
%
 this mapping naively is described in \cref{srmpds}, and in \cref{cc,tocs} we 



Once mapped, remote resources are accessed over standard \gls{pcie}.
%
However, 
%
%However, there are some caveats performance considerations regarding \gls{pcie} paths and \glspl{iommu}.
experiments



\objdynamic*
%
Our SmartIO driver, described in \cref{sec:smartio-driver}, is able to locate devices and \glspl{memorysegment} in the cluster and determine the shortest path between them.




\objdisaggregation*%
        %
        Device memory regions are exported as \glspl{sharedsegment} that can be mapped directly into local address space, similar to how a local application would map device memory on a local system.
        %
        \Glspl{sharedsegment} can be mapped for devices, making it possible for devices to read from and write to these \glspl{segment} using native \gls{dma}.

    dl mdev = more general than rdma, any pcie device shared and appear local
mriov in software

% Refer back to the main research question
% How does this move the world forward?


\section{Future work}\label{sec:fw}

mention new NVMe kernel space driver here

security / safety

disaggregated memory - new interconnects

iommu in tree structures is a challenge - ats? other solutions?


scaling

%With SmartIO, the hard separation between local and remote is blurred, as remote resources can be used as if they were locally installed and with native PCIe performance.


%Using the \gls{sisciapi}, application memory can be exported as \glspl{sharedsegment}, and \glspl{segment} in remote machines can be mapped into a local application process' virtual address space.
%By building on these concepts, our \lgls{apiext}{extension} makes it possible for a device driver implementation to use all the memory \gls{disaggregation} capabilities of \gls{sisci}, while also providing functionality for abstracting away the location of memory resources and resolving addresses between different address spaces.
%
%A device driver implemented using our \gls{apiext} can be agnostic about the underlying \gls{pcie} network topology, as devices may \gls{dma} directly to \glspl{sharedsegment}, regardless of whether they are local or remote.
%
%It is even possible to map device memory of other devices registered with SmartIO, for example devices borrowed using \gls{dl}.
