\chapter{Conclusion}\label{chapter:conclusion}
As distributed and parallel computing applications are becoming increasingly compute-intensive and data-driven, \gls{io} performance demands are ever growing.
%
Computing accelerators (such as \glspl{fpga} and \glspl{gpu}), high-throughput \glspl{nic}, and fast storage devices like \glspl{nvme}, are now commonplace in most modern computer systems.
%
Nevertheless, distributing such \gls{io} resources in a way that maximizes both performance and resource utilization is a challenge for heterogeneous computing clusters. 
%
To avoid that individual machines becoming performance bottlenecks, resources must be shared efficiently between machines in the cluster.



In this dissertation, we have addressed this challenge and presented our SmartIO framework for sharing \gls{io} resources between machines connected over \gls{pcie}.
%
Our SmartIO framework effectively makes all machines, including their internal devices and memory, part of a common \gls{pcie} domain.
%
Resources in remote machines can be used as if they were installed locally, without any performance degradation compared to local access, and without requiring adaptions to device drivers or application software.
%
The hard separation between local and remote is blurred out, as machines can freely share their internal devices and memory resources with other machines in the cluster. 



\section{Summary}\label{sec:summary}
% - What was the target problem?
Connecting two or more computer systems over \gls{pcie} is possible by using \glspl{pcientb}.
%
\Glspl{ntb} have memory address translation capabilities that makes it possible for a machine to map \glspl{segment} of remote memory directly into local address space.
%
However, leveraging \glspl{ntb} to share the internal devices and memory of a machine with other, remote machines is a challenge, as the use of a remote resource requires software to be aware of the fact that the resource is on the other side of an \gls{ntb}.
%
For example, a device driver operating a remote device must use addresses that correspond to the remote device's address space when initiating \gls{dma} transfers or configuring interrupts.
%
This additional complexity makes it infeasible to rely on \glspl{ntb} alone to implement a resource sharing solution, as it would require extensive modifications to existing software.



% - What did you develop?
To solve this, we have developed our SmartIO framework for sharing devices and memory resources between machines connected with \glspl{ntb}.
%
Our solution consists of ``\glspl{lender}'', machines lending out one or more of its internal devices, and ``\glspl{borrower}'', machines using such a device.
%
Machines can act as \gls{lender} and \gls{borrower} at the same time, making SmartIO fully distributed.
%
Any type of \gls{pcie} device may be shared, as SmartIO is built on standard \gls{pcie}.
%
SmartIO keeps track of which machines devices and \glspl{memorysegment} reside in, and is able to map resources on behalf of devices and resolve memory addresses as they are seen by devices.
%
As such, SmartIO provides a logical decoupling of devices and which \glspl{lendermachine} they are installed in, solving the challenge of managing multiple address spaces and making remote resources appear and behave as if they are local.




SmartIO supports three different methods of device sharing:
%
\begin{itemize}
    \item Our \textbf{\gls{dl}} sharing method makes it possible to dynamically assign a \gls{pcie} device to a remote \gls{borrowermachine}.
        %
        %By using a ``\gls{shadowdev}'', the device appears \gls{hotadded} to the local device tree on the \gls{borrower}.
        %
        The fact that the device is remote is made transparent to the system, allowing the device to be used by native device drivers and application software as if it was locally installed.


    \item Our \textbf{\gls{mdev}} extension to the \gls{kvm} makes it possible to distribute devices to \glspl{vm} running on remote machines, by facilitating \emph{\gls{passthrough}} of a device to the \gls{vmguest}.
        %
        Application software and device drivers running inside the \gls{vmguest} can directly interact the physical device, without compromising the isolation of the virtualized environment.


    \item Our \textbf{\gls{sisciapiext}} makes it possible to \gls{disaggregate} devices and memory resources in software.
        %
        %We have extended the \gls{sisciapi} with device-oriented programming semantics and device driver support functionality, making core SmartIO capabilities available through the same shared-memory \gls{api} used to write cluster applications.
        %
        Using this \gls{apiext}, we have also implemented a \textbf{proof-of-concept \gls{nvme} driver} that demonstrates sharing \glspl{nvme} with multiple machines at the same time.
        
\end{itemize}




% - What were the results?
We have performed an extensive performance evaluation, consisting of a comprehensive collection of synthetic performance benchmarking and realistic workloads.
%
We have made a point out of using standard benchmarking software and device drivers, as well as a wide variety of \gls{pcie} devices, in order to demonstrate the completeness of our SmartIO framework.
%
Particularly, we have performed comparison tests where we compare the performance of a workload using remote resources to the same workload running only on a local system.
%
The results prove that, when conditions are similar, the SmartIO sharing methods \textbf{do not add \emph{any} performance overhead} compared to using local resources.
%
Furthermore, we have also explored how different network topologies affect the performance, and have identified situations where the \gls{iommu} can become a potential performance bottleneck.
%
Finally, our exhaustive performance test suite also includes tests using our proof-of-concept \gls{nvme} driver that highlights possibilities that are enabled by our shared-memory approach to device sharing.






\section{Revisiting the problem statement}\label{sec:discussion}
The main goal of this dissertation was to use \glspl{ntb} to develop a solution that allows the internal \gls{io} resources of machines to be shared with, and used by, remote machines in a cluster, as if these resources were local to the remote machines using them.
%
In \cref{sec:problem}, we broke down the challenges of this goal into six objectives: %\crefrange*{obj:distributed}{obj:experiments}:



% How did we answer objectives 1-6? 
% For each objective, state clearly how it was solved and the contribution of it (refer back to contributions in 1.5)
% Also make it clear how the contribution helps solve the overall research question as well.



\objdistributed*%
%
%Using SmartIO, machines act as ``\emph{\glspl{lender}}'' and ``\emph{\glspl{borrower}}''. 
%
%A \gls{lender} registers one or more of its devices with SmartIO, allowing these devices to be used by remote machines.
%%
%A \gls{borrower} is a system or software process that is currently using such a device.
%
SmartIO is fully distributed, allowing \emph{any} machine in the cluster to act as a ``\emph{\gls{lender}}'' or a ``\emph{\gls{borrower}}'', or even acting as both at the same time.
%
Any \gls{pcie} device may be registered with SmartIO and shared, as demonstrated by our comprehensive performance evaluation in \cref{tocs}.
%
As such, we enable a peer-to-peer sharing model, where all machines in the cluster can participate in the sharing through contributing their own resources and using resources shared by others.


We implemented three different sharing methods for our solution: 
%
\begin{itemize}
    \item The \gls{dl} sharing method, explained in \cref{sec:lending}, makes it possible to distribute devices to remote machines.
        %
        The initial \gls{dl} method is presented in \cref{nossdav}. Subsequent improvements are presented in \cref{srmpds,cc,tocs}.



    \item The \gls{mdev} extension to the \gls{kvm} makes it possible to distribute devices to \glspl{vm} running on remote machines, as detailed in \cref{sec:mdev}.
        %
        The initial \gls{mdev} method is presented in \cref{srmpds}, and improved versions are presented in \cref{cc,tocs}.


    \item The \gls{apiext} brings device-oriented programming semantics and device driver support functions to the \gls{sisciapi}.
        %
        Using the \gls{apiext}, \gls{userspace} device drivers can be implemented using the same \gls{api} used to implement shared-memory communication using \glspl{ntb}, as explained in \cref{sec:api}.
        %
        The \gls{apiext} is presented in \cref{tocs}.
\end{itemize}
%
These sharing capabilities set SmartIO apart from existing \gls{pcie}-based \gls{disaggregation} solutions (including Ladon~\cite{Tu2013}), as these solutions are only able to share devices in dedicated servers.
%
Thus, our sharing methods solves \cref*{obj:distributed}.





\objtransparent*%
%
Our three sharing methods address this objective in the following ways:
%
\begin{itemize}
    \item \Gls{dl} inserts a remote device into the local device tree of the \gls{host}~\gls{os} by using a ``\gls{shadowdev}''.
        %
        This allows device drivers, application software, and even the (\gls{host})~\gls{os} itself to use the remote device through \emph{native} \gls{os} interfaces, in the same way they would use a local device.
        %
        No adaptations to existing software is required.
        %
        This is further explained \gls{dl} in \cref{nossdav,srmpds,cc,tocs}.
    


    \item \Gls{mdev} enables \gls{passthrough} of a remote device to a \gls{vm}.
        %
        Software running in the \gls{guest}, including device drivers and the \gls{guest}~\gls{os}, may interact with the physical device directly, as if the device was locally installed.
        %
        No modifications to \gls{vmemulator} software or \gls{host}~\gls{os} is necessary.
        %
        \Gls{mdev} is described in further detail in \cref{srmpds,cc,tocs}.


    \item Using the \gls{sisciapi}, remote \glspl{memorysegment} are mapped directly into the virtual address space of a local application.
        %
        Our \gls{exttosisciapi} makes it possible to map such \glspl{segment} for \emph{devices} as well.
        %
        This enables native \gls{dma} to remote memory resources, as if both the device and the memory being accessed were both installed in the same, local machine.
        %
        Moreover, using the \gls{apiext}, the physical location of both devices and \glspl{memorysegment} are abstracted away.
        %
        \Gls{userspace} device drivers implemented using our \gls{ext} can be written as if all resources are local, similarly to how a local \gls{userspace} device driver (for a local device) would be implemented.
        %
        The \gls{apiext} is described in \cref{tocs}.
\end{itemize}
%
Whether resources are remote or local is made transparent by SmartIO, as remote devices and memory resources both appear and behave as if they are locally installed.
%
In this regard, SmartIO differs from existing \gls{disaggregation} solutions based on \gls{rdma}.
%
Contrary to these solutions, we do not require interacting with a device driver running on the remote system, thus avoiding any \glspl{middlewareservice} or specialized adaptations to existing software.
%
Scaling out becomes significantly easier, as SmartIO allows remote resources to be used natively instead.
%
Thus, this aspect of SmartIO solves \cref*{obj:transparent}.



\objperformance*%
%
%With SmartIO, remote resources can be mapped using the \gls{ntb} and accessed over standard \gls{pcie}.
%
One of the main challenges for our \gls{dl} and \gls{mdev} sharing methods was that local \gls{ram} must be mapped ahead of time in order to avoid communication overhead in the performance-critical path, yet memory used by a device driver can not be known in advance.
%
To overcome this, our SmartIO implementation supports using the \gls{borrower}'s \gls{iommu} to create continuous memory ranges that can be mapped as ``\glspl{dmawindow}'' through the \gls{lender}'s \glspl{ntb} before use.
%
Memory pages can then be dynamically added and removed from these \gls{iommu} ranges locally on the \gls{borrower}, and communication with a remote system in the critical path is avoided.



Once mapped, remote resources are accessed with native \gls{pcie} performance, as all address translations are done in \gls{ntb} (and \gls{iommu}) hardware.
%
In fact, our evaluation in \cref{tocs} prove that, when conditions are similar, SmartIO allows remote resources to be used \emph{without any performance overhead} compared to using local resources.
%
Nevertheless, using remote resources may lead to a longer distance between resources.
%
As such, there are some caveats that must be considered:
%
\begin{itemize}
    \item Longer \gls{pcie} paths affect \gls{dma} performance, particularly \gls{dma} reads, as we uncovered in \cref{srmpds,cc,tocs}.
        %
        This remains an unsolved challenge for \gls{dl} or \gls{mdev}, as we have no control over the memory allocated by a device driver in these instances.
        %
        Therefore, we recommend considering the length of \gls{pcie} paths when designing the cluster.
        %
        The issue of longer \gls{pcie} paths affects drivers implemented using our \gls{sisciapiext} to a lesser extent;
        %
        by using memory access pattern hinting when allocating \gls{dma} buffers, SmartIO will attempt to minimize the distance a device or a \gls{cpu} needs to read across.
        %
        The performance experiment presented in \cref{sec:eval-nvme} demonstrates this.


    \item Our performance experiments in \cref{srmpds,cc,tocs} also revealed that an \gls{iommu} in the data path can negatively affect \gls{dma} performance, as the \gls{iommu} may split large \gls{pcie} transactions into several, smaller-sized transactions.
        %
        This is especially an issue for our \gls{mdev} sharing method, as SmartIO uses the \gls{lender}'s \gls{iommu} in order to map the device to the same \gls{guestphys} address space as the \gls{vm} the device is \lgls{passthrough}{passed-through} to.
        %
        For \gls{dl}, the use of an \gls{iommu} on the \gls{lender} is optional.
        %
        However, the use of an \gls{iommu} on the \gls{borrower} is necessary (except in a few scenarios where it is possible to map the entire \gls{ram} of the \gls{borrower}). 
        %
        Consequently, this may introduce limitations on scenarios where machines act as both \glspl{lender} and \glspl{borrower}, where maximizing \gls{dma} performance is a requirement.
        %
        In the case where device drivers are implemented using the \gls{apiext}, an \gls{iommu} is entirely optional on \emph{both} the \gls{lender} and the \gls{borrower}.
\end{itemize}
%
By making it possible for remote resources to be accessed over native \gls{pcie}, \cref*{obj:performance} is solved.
%
Improving performance issues involving \glspl{iommu} is a candidate for future work.



\objdynamic*%
%
%\Glspl{lender} may even forcefully reclaim their devices, should it be necessary.
%
%
Using SmartIO, resources may be shared without requiring machines to be rebooted.
%
Devices registered with SmartIO can be borrowed by any machine, at any time, using any of the three sharing methods.
%
For example, a machine may borrow a device using \gls{dl} and at the same time run a \gls{vm} that is borrowing another device using \gls{mdev}.
%
The different sharing methods can also be combined, as demonstrated by the proof-of-concept NVMe driver experiment presented in \cref{sec:eval-nvme}.
%
When the device is no longer needed, it can be returned so it may be used by another \gls{borrower}.
%
Through borrowing and returning devices, systems may dynamically scale \gls{io} resources up or down based on current workload demands.



Devices are logically decoupled from the machines they are physically installed in, allowing software to be moved to any machine in the cluster.
%
SmartIO keeps track of both \glspl{memorysegment} and devices, and is able to locate resources in the cluster, without requiring that the user knows anything about the underlying \gls{pcie} topology.
%
The shortest path between devices, \glspl{cpu}, and \glspl{memorysegment} is determined automatically, and SmartIO configures \glspl{ntb} along that path in order to map remote memory resources for \glspl{cpu} and devices.
%
Moreover, SmartIO also supports borrowing devices from multiple \glspl{lender} and enabling \gls{p2pdma} transfers between them, as we explain in \cref{srmpds,cc,tocs}.
%
\Gls{p2p} can be enabled when borrowing devices using \gls{dl} or \gls{mdev}, which is demonstrated in the various \gls{p2p} experiments presented in these papers.
%
\Gls{p2p} is also supported when using the \gls{apiext}, which we demonstrate in the proof-of-concept \gls{nvme} driver experiment (\cref{sec:eval-nvme}).
%
Our various performance experiments demonstrate that SmartIO is a dynamic and flexible sharing framework, thus solving \cref*{obj:dynamic}.




\objdisaggregation*%
%
SmartIO is able to \gls{disaggregate} multi-function devices, such as devices capable of \gls{sriov}, and distribute individual \glspl{devicefunction} to different \glspl{borrower}.
%
An experiment demonstrating this is presented in \cref{tocs}.
%
Devices that do not support \gls{sriov} may be \gls{disaggregated} in \emph{software} instead, using our \gls{exttosisciapi}.
%
Using the \gls{apiext}, a device be borrowed by several machines simultaneously.
%
Our proof-of-concept \gls{nvme} driver presented in \cref{tocs} demonstrate this, where several \glspl{borrower} share the same (non-\gls{sriov}) \gls{nvme}.
%
In other words, the \gls{apiext} enables ``\gls{mriov} in software''.



The \gls{apiext} makes it possible to implement device drivers as part of distributed, shared-memory cluster applications. 
%
Any \gls{memorysegment} anywhere in the cluster can be mapped for devices, so they may access them directly, including \glspl{segment} in local \gls{ram} on the \gls{borrower}, \glspl{segment} in \gls{ram} on the \gls{lender}, and even \glspl{segment} in memory of a different cluster machine altogether.
%
\Glspl{devicebar} are also automatically exported by SmartIO as \glspl{sharedsegment}, allowing device memory to be mapped for the application process or even for \emph{other devices} (thus enabling \gls{p2p}).
%
As such, SmartIO supports \gls{disaggregating} device memory.
%
It is even possible to map \gls{multicasting} \glspl{segment} for a device, allowing a device to stream data to multiple destinations in a single operation.
%
Moreover, SmartIO makes it possible to associate \glspl{memorysegment} with a device (rather than a machine in the cluster), allowing the location of \glspl{memorysegment} to be abstracted away in a similar fashion to devices.
%
This allows software to be written as if all resources are local, and can run on any machine in the cluster.
%
%Not only does this allow software to be moved to any machine in the cluster, but the implementation of device drivers becomes easier as well as they can be written as if all resources are local.
%
SmartIO is able to optimize memory locations without requiring that the user is aware of the underlying \gls{pcie} network topology.
%
The proof-of-concept \gls{nvme} driver experiment presented in \cref{sec:eval-nvme} demonstrates all of these capabilities, proving that \cref*{obj:disaggregation} is solved.



\objexperiments*%
%
To prove that SmartIO is an efficient solution for real-world applications, we have performed a comprehensive performance evaluation consisting of both synthetic microbencmarks and realistic, large-scale workloads.
%
All parts of our SmartIO implementation have been evaluated, and we have included several sharing scenarios and network topologies have been evaluated, as well as a wide range of standard and commodity \gls{pcie} devices like \glspl{gpu}, \glspl{nic}, and \glspl{nvme}.
%
As SmartIO makes it possible to use remote devices as if they were local, we have used standard and unmodified benchmarking tools and device drivers.
%
Through comparison testing, we prove that SmartIO does not add any performance overhead compared to using local resources, in terms of latency and throughput.
%
We have also explored the performance effects of moving resources further away, and present a thourough analysis of this.
%
Finally, a proof-of-concept \gls{nvme} driver was developed in order to evaluate our \gls{apiext} and the \gls{disaggregation} possibilities enabled by SmartIO.
%
All of the published papers contributed towards solving this objective (\crefrange{nossdav}{tocs}).




% Refer back to the main research question
% How does this move the world forward?
By solving all of our six research objectives, we have answered the central research question of this dissertation:
\researchquestion*%
%%
We have not just implemented yet another \gls{disaggregation} solution, but developed a new and more flexible solution by taking a novel approach:
%
utilizing the memory mapping capabilities of \glspl{ntb} to unify traditional device \gls{io} with distributed, shared-memory computing.
%
Our implementation makes it possible for machines to share their inner devices and memory with other machines in a \gls{pcie} cluster.
%
Remote resources can be used over standard \gls{pcie}, making our SmartIO framework a zero-overhead solution for scaling out and transparently using more hardware resources than there are available in a single machine.
%
By lending out their own local resources and borrowing remote resources, machines take part in a dynamic and composable \gls{io} infrastructure.
%Through a process of borrowing and returning devices, machines take part in a dynamic and composable \gls{io} infrastructure.
%
Thus, we have shown that we can leverage \glspl{ntb} to develop a solution where resources can be freely shared in a \gls{pcie}-networked cluster.



\section{Future work}\label{sec:fw}
%, particularly implementing support for \gls{ats}~\cite{spec:ATS}, which allows devices (and \gls{pcie} switch chips) to cache resolved \gls{io}~addresses.
%
%However, as \gls{ats} requires support in both devices and \glspl{iommu}, it appears not to be widely adopted, especially for commodity hardware.
%

mention new NVMe kernel space driver here

security / safety

disaggregated memory - new interconnects
%gen z interconnect data center level, cxl -> rack interconnect

iommu in tree structures is a challenge - ats? other solutions?

migrating vms

scaling 

%With SmartIO, the hard separation between local and remote is blurred, as remote resources can be used as if they were locally installed and with native PCIe performance.


%Using the \gls{sisciapi}, application memory can be exported as \glspl{sharedsegment}, and \glspl{segment} in remote machines can be mapped into a local application process' virtual address space.
%By building on these concepts, our \lgls{apiext}{extension} makes it possible for a device driver implementation to use all the memory \gls{disaggregation} capabilities of \gls{sisci}, while also providing functionality for abstracting away the location of memory resources and resolving addresses between different address spaces.
%
%A device driver implemented using our \gls{apiext} can be agnostic about the underlying \gls{pcie} network topology, as devices may \gls{dma} directly to \glspl{sharedsegment}, regardless of whether they are local or remote.
%
%It is even possible to map device memory of other devices registered with SmartIO, for example devices borrowed using \gls{dl}.

%
%
%%Borrowing and returning devices is dynamic, as SmartIO distributes devices while all systems are running:
%%
%\begin{itemize}
%    \item Our \gls{dl} sharing method \gls{hotadds} devices to a local system by using a \gls{shadowdev}, as outlined in \cref{nossdav,cc,tocs}.
%        %
%        When a device is returned (or reclaimed), the \gls{shadowdev} is hot-removed.
%        %
%        The \gls{shadowdev} also makes it possible to detect when a device driver is loaded and allocates \gls{dma} buffers, in order to map them for the device. 
%        %
%        This way, \gls{dl} is able to dynamically distribute devices to physical \glspl{hostmachine} at run-time, without requiring a reboot or \gls{bios} (re-)configuration is needed.
%
%
%    \item Devices can also be distributed to \glspl{vm} using the \gls{mdev} sharing method.
%        %
%        %\glspl{vm} can be migrated to any machine in the cluster.
%        %
%        Devices assigned to a \gls{vm} are automatically borrowed when the \gls{vmguest} boots, and returned when the \gls{vm} shuts down.
%        %
%        It is also possible to \gls{hotadd} devices to a running \gls{vm}, if the \gls{vmemulator} supports this for \gls{passthrough} devices.
%        %
%        As outlined in \cref{cc,tocs}, we probe the \gls{kvm} to dynamically detect the \gls{guest}'s memory layout, which is then used when mapping the device to the same \gls{guestphys} address space as the \gls{vm}.
%        %
%
%
%    \item Our \gls{sisciapiext} provides functions for borrowing and returning devices, mapping \glspl{sharedsegment} for devices, and mapping \glspl{devicebar} for the application process or for other devices.
%        %
%        A device is borrowed (or returned) at any point in the program being executed, as determined by the programmer.
%        %
%        These \gls{api} functions are described in further detail in \paperref{tocs:api}.
%\end{itemize}
%%
%
%
%As explained in \cref{sec:smartio-driver}, we use device identifiers to locate devices.
%%
%SmartIO keeps track of both \glspl{sharedsegment} and devices, and is able to look up \glspl{lendermachine} and which \glspl{ntb} to use in order to reach a device or a \gls{segment}. 
%%
%This allows SmartIO to determine the shortest path to a resource in the cluster, and configure \glspl{ntb} along that path.
%%
%Thus, borrowing and returning devices is made dynamic:
%%
%\begin{itemize}
%    \item
%        Our \gls{dl} method distributes devices to physical \glspl{hostmachine}.
%        %
%        As we explain in \cref{nossdav,cc,tocs}, a remote device appears \gls{hotadded} to the local system by inserting a \gls{shadowdev} into the local \gls{pcie} device tree.
%        %
%        When the device is borrowed, SmartIO configures a \gls{dmawindow} to a local \gls{iommu} range.
%        %
%        The \gls{shadowdev} intercepts when a device driver on the \gls{borrower} allocates \gls{dma} buffers, in order to add memory pages to the local \gls{iommu} range.
%        %
%        This way, we are able to detect system memory used by a device driver running on the \gls{borrower} and map it for the device.
%        %
%        When a device is returned (or reclaimed), the \gls{shadowdev} is removed from the local device tree and the device appears hot-removed.
%        %
%        The \gls{shadowdev} is added and removed dynamically while the system is running, and no reboot is required.
%
%
%    \item Our \gls{mdev} method distributes devices to \glspl{vm}.
%        %
%        Using device identifiers, we can assign device identifiers to \glspl{vm} as part of their configuration.
%        %
%        When a \gls{vm} boots up, any assigned devices are located in the cluster by SmartIO, and automatically borrowed. 
%        %
%        As outlined in \cref{cc,tocs}, our \gls{mdev} implementation is able to detect the \gls{vm}'s memory layout by probing \gls{kvm}.
%        %
%        This way, we are able to configure \glspl{dmawindow} and map devices to the \gls{guestphys} address space by using the \gls{lender}'s \gls{iommu}.
%        %
%        Devices remain borrowed as long as the \gls{vm} is running, and are returned when the \gls{vm} shuts down.
%        %
%        This dynamic borrowing and returning of devices allows \glspl{vm} to be migrated to any machine in the cluster.
%        %
%        It is also possible to \gls{hotadd} devices to a running \gls{vm}, if the \gls{vmemulator} supports this for \gls{passthrough} devices.
%        
%
%    \item Using our \gls{sisciapiext}, borrowing and returning devices is done explicitly through the \gls{api}.
%        %
%        A device is borrowed (or returned) at any point in the program being executed, as determined by the programmer.
%        %
%        Mapping \glspl{sharedsegment} for a device is similarly explicit, as our extension provides \gls{api} functions for mapping a \gls{segment} for a device.
%        %
%        Any \gls{segment} anywhere in the cluster can be mapped for a device, including \glspl{segment} in local \gls{ram} on the borrower, \glspl{segment} in \gls{ram} on the \gls{lender}, and even \glspl{segment} in memory of a different cluster machine altogether.
%        %
%        It is even possible to map \glspl{bar} of other devices for a device, as \glspl{devicebar} are automatically exported as \glspl{sharedsegment} by SmartIO.
%        %
%        When the program calls the mapping functions, SmartIO automatically resolves the path between the device and (remote) \glspl{memorysegment}, and maps the \glspl{segment} for the device through the appropriate \glspl{ntb}.
%        %
%        Because of this, a device driver implemented using the \gls{apiext} can be moved to any machine in the cluster.
%        %
%        Our proof-of-concept \gls{nvme} driver, presented in \cref{tocs}, is an example of this, as the same application process can run on any machine in the cluster.
%\end{itemize}
%\footnote{Many \glspl{vmemulator} support \gls{hotadding} \emph{virtual} devices to running \gls{vm}, but few support this for \emph{\gls{passthrough}} devices.}

%
