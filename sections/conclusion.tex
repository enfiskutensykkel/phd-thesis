\chapter{Conclusion}\label{chapter:conclusion}
As distributed and parallel computing applications are becoming increasingly compute-intensive and data-driven, \gls{io} performance demands are ever growing.
%
Computing accelerators (such as \glspl{fpga} and \glspl{gpu}), high-throughput \glspl{nic}, and fast storage devices like \glspl{nvme}, are now commonplace in most modern computer systems.
%
Nevertheless, distributing such \gls{io} resources in a way that maximizes both performance and resource utilization is a challenge for heterogeneous computing clusters. 
%
To avoid that individual machines becoming performance bottlenecks, resources must be shared efficiently between machines in the cluster.



In this dissertation, we have addressed this challenge and presented our SmartIO framework for sharing \gls{io} resources between machines connected over \gls{pcie}.
%
Our SmartIO framework effectively makes all machines, including their internal devices and memory, part of a common \gls{pcie} domain.
%
Resources in remote machines can be used as if they were installed locally, without any performance degradation compared to local access, and without requiring adaptions to device drivers or application software.
%
The hard separation between local and remote is blurred out, as machines can freely share their internal devices and memory resources with other machines in the cluster. 



\section{Summary}\label{sec:summary}
% - What was the target problem?
Connecting two or more computer systems over \gls{pcie} is possible by using \glspl{pcientb}.
%
\Glspl{ntb} have memory address translation capabilities that makes it possible for a machine to map \glspl{segment} of remote memory directly into local address space.
%
However, leveraging \glspl{ntb} to share the internal devices and memory of a machine with other, remote machines is a challenge, as the use of a remote resource requires software to be aware of the fact that the resource is on the other side of an \gls{ntb}.
%
For example, a device driver operating a remote device must use addresses that correspond to the remote device's address space when initiating \gls{dma} transfers or configuring interrupts.
%
This additional complexity makes it infeasible to rely on \glspl{ntb} alone to implement a resource sharing solution, as it would require extensive modifications to existing software.



% - What did you develop?
To solve this, we have developed our SmartIO framework for sharing devices and memory resources between machines connected with \glspl{ntb}.
%
Our solution consists of ``\glspl{lender}'', machines lending out one or more of its internal devices, and ``\glspl{borrower}'', machines using such a device.
%
Machines can act as \gls{lender} and \gls{borrower} at the same time, making SmartIO fully distributed.
%
Any type of \gls{pcie} device may be shared, as SmartIO is built on standard \gls{pcie}.
%
SmartIO keeps track of which machines devices and \glspl{memorysegment} reside in, and is able to map resources on behalf of devices and resolve memory addresses as they are seen by devices.
%
As such, SmartIO provides a logical decoupling of devices and which \glspl{lendermachine} they are installed in, solving the challenge of managing multiple address spaces and making remote resources appear and behave as if they are local.




SmartIO supports three different methods of device sharing:
%
\begin{itemize}
    \item Our \textbf{\gls{dl}} sharing method makes it possible to dynamically assign a \gls{pcie} device to a remote \gls{borrowermachine}.
        %
        %By using a ``\gls{shadowdev}'', the device appears \gls{hotadded} to the local device tree on the \gls{borrower}.
        %
        The fact that the device is remote is made transparent to the system, allowing the device to be used by native device drivers and application software as if it was locally installed.


    \item Our \textbf{\gls{mdev}} extension to the \gls{kvm} makes it possible to distribute devices to \glspl{vm} running on remote machines, by facilitating \emph{\gls{passthrough}} of a device to the \gls{vmguest}.
        %
        Application software and device drivers running inside the \gls{vmguest} can directly interact the physical device, without compromising the isolation of the virtualized environment.


    \item Our \textbf{\gls{sisciapiext}} makes it possible to \gls{disaggregate} devices and memory resources in software.
        %
        %We have extended the \gls{sisciapi} with device-oriented programming semantics and device driver support functionality, making core SmartIO capabilities available through the same shared-memory \gls{api} used to write cluster applications.
        %
        Using this \gls{apiext}, we have also implemented a \textbf{proof-of-concept \gls{nvme} driver} that demonstrates sharing \glspl{nvme} with multiple machines at the same time.
        
\end{itemize}




% - What were the results?
We have performed an extensive performance evaluation, consisting of a comprehensive collection of synthetic performance benchmarking and realistic workloads.
%
We have made a point out of using standard benchmarking software and device drivers, as well as a wide variety of \gls{pcie} devices, in order to demonstrate the completeness of our SmartIO framework.
%
Particularly, we have performed comparison tests where we compare the performance of a workload using remote resources to the same workload running only on a local system.
%
The results prove that, when conditions are similar, the SmartIO sharing methods \textbf{do not add \emph{any} performance overhead} compared to using local resources.
%
Furthermore, we have also explored how different network topologies affect the performance, and have identified situations where the \gls{iommu} can become a potential performance bottleneck.
%
Finally, our exhaustive performance test suite also includes tests using our proof-of-concept \gls{nvme} driver that highlights possibilities that are enabled by our shared-memory approach to device sharing.






\section{Revisiting the problem statement}\label{sec:discussion}
The main goal of this dissertation was to use \glspl{ntb} to develop a solution that allows the internal \gls{io} resources of machines to be shared with, and used by, remote machines in a cluster, as if these resources were local to the remote machines using them.
%
In \cref{sec:problem}, we broke down the challenges of this goal into six objectives: %\crefrange*{obj:distributed}{obj:experiments}:



% How did we answer objectives 1-6? 
% For each objective, state clearly how it was solved and the contribution of it (refer back to contributions in 1.5)
% Also make it clear how the contribution helps solve the overall research question as well.



\objdistributed*%
%
%Using SmartIO, machines act as ``\emph{\glspl{lender}}'' and ``\emph{\glspl{borrower}}''. 
%
%A \gls{lender} registers one or more of its devices with SmartIO, allowing these devices to be used by remote machines.
%%
%A \gls{borrower} is a system or software process that is currently using such a device.
%
SmartIO is fully distributed, allowing \emph{any} machine in the cluster to act as a ``\emph{\gls{lender}}'' or a ``\emph{\gls{borrower}}'', or even acting as both at the same time.
%
Any \gls{pcie} device may be registered with SmartIO and shared, as demonstrated by our comprehensive performance evaluation in \cref{tocs}.
%
As such, we enable a peer-to-peer sharing model, where all machines in the cluster can participate in the sharing through contributing their own resources and using resources shared by others.


We implemented three different sharing methods for our solution: 
%
\begin{itemize}
    \item The \gls{dl} sharing method, explained in \cref{sec:lending}, makes it possible to distribute devices to remote machines.
        %
        The initial \gls{dl} method is presented in \cref{nossdav}. Subsequent improvements are presented in \cref{srmpds,cc,tocs}.



    \item The \gls{mdev} extension to the \gls{kvm} makes it possible to distribute devices to \glspl{vm} running on remote machines, as detailed in \cref{sec:mdev}.
        %
        The initial \gls{mdev} method is presented in \cref{srmpds}, and improved versions are presented in \cref{cc,tocs}.


    \item The \gls{apiext} brings device-oriented programming semantics and device driver support functions to the \gls{sisciapi}.
        %
        Using the \gls{apiext}, \gls{userspace} device drivers can be implemented using the same \gls{api} used to implement shared-memory communication using \glspl{ntb}, as explained in \cref{sec:api}.
        %
        The \gls{apiext} is presented in \cref{tocs}.
\end{itemize}
%
These sharing capabilities set SmartIO apart from existing \gls{pcie}-based \gls{disaggregation} solutions (including Ladon~\cite{Tu2013}), as these solutions are only able to share devices in dedicated servers.
%
Thus, our sharing methods solves \cref*{obj:distributed}.





\objtransparent*%
%
Our three sharing methods address this objective in the following ways:
%
\begin{itemize}
    \item \Gls{dl} inserts a remote device into the local device tree of the \gls{host}~\gls{os} by using a ``\gls{shadowdev}''.
        %
        This allows device drivers, application software, and even the (\gls{host})~\gls{os} itself to use the remote device through \emph{native} \gls{os} interfaces, in the same way they would use a local device.
        %
        No adaptations to existing software is required.
        %
        This is further explained \gls{dl} in \cref{nossdav,srmpds,cc,tocs}.
    


    \item \Gls{mdev} enables \gls{passthrough} of a remote device to a \gls{vm}.
        %
        Software running in the \gls{guest}, including device drivers and the \gls{guest}~\gls{os}, may interact with the physical device directly, as if the device was locally installed.
        %
        No modifications to \gls{vmemulator} software or \gls{host}~\gls{os} is necessary.
        %
        \Gls{mdev} is described in further detail in \cref{srmpds,cc,tocs}.


    \item Using the \gls{sisciapi}, remote \glspl{memorysegment} are mapped directly into the virtual address space of a local application.
        %
        Our \gls{exttosisciapi} makes it possible to map such \glspl{segment} for \emph{devices} as well.
        %
        This enables native \gls{dma} to remote memory resources, as if both the device and the memory being accessed were both installed in the same, local machine.
        %
        Moreover, using the \gls{apiext}, the physical location of both devices and \glspl{memorysegment} are abstracted away.
        %
        \Gls{userspace} device drivers implemented using our \gls{ext} can be written as if all resources are local, similarly to how a local \gls{userspace} device driver (for a local device) would be implemented.
        %
        The \gls{apiext} is described in \cref{tocs}.
\end{itemize}
%
Whether resources are remote or local is made transparent by SmartIO, as remote devices and memory resources both appear and behave as if they are locally installed.
%
In this regard, SmartIO differs from existing \gls{disaggregation} solutions based on \gls{rdma}.
%
Contrary to these solutions, we do not require interacting with a device driver running on the remote system, thus avoiding any \glspl{middlewareservice} or specialized adaptations to existing software.
%
Scaling out becomes significantly easier, as SmartIO allows remote resources to be used natively instead.
%
Thus, this aspect of SmartIO solves \cref*{obj:transparent}.



\objperformance*%
%
%With SmartIO, remote resources can be mapped using the \gls{ntb} and accessed over standard \gls{pcie}.
%
One of the main challenges for our \gls{dl} and \gls{mdev} sharing methods was that local \gls{ram} must be mapped ahead of time in order to avoid communication overhead in the performance-critical path, yet memory used by a device driver can not be known in advance.
%
To overcome this, our SmartIO implementation supports using the \gls{borrower}'s \gls{iommu} to create continuous memory ranges that can be mapped as ``\glspl{dmawindow}'' through the \gls{lender}'s \glspl{ntb} before use.
%
Memory pages can then be dynamically added and removed from these \gls{iommu} ranges locally on the \gls{borrower}, and communication with a remote system in the critical path is avoided.



Once mapped, remote resources are accessed with native \gls{pcie} performance, as all address translations are done in \gls{ntb} (and \gls{iommu}) hardware.
%
In fact, our evaluation in \cref{tocs} prove that, when conditions are similar, SmartIO allows remote resources to be used \emph{without any performance overhead} compared to using local resources.
%
Nevertheless, using remote resources may lead to a longer distance between resources.
%
As such, there are some caveats that must be considered:
%
\begin{itemize}
    \item Longer \gls{pcie} paths affect \gls{dma} performance, particularly \gls{dma} reads, as we uncovered in \cref{srmpds,cc,tocs}.
        %
        This remains an unsolved challenge for \gls{dl} or \gls{mdev}, as we have no control over the memory allocated by a device driver in these instances.
        %
        Therefore, we recommend considering the length of \gls{pcie} paths when designing the cluster.
        %
        The issue of longer \gls{pcie} paths affects drivers implemented using our \gls{sisciapiext} to a lesser extent;
        %
        by using memory access pattern hinting when allocating \gls{dma} buffers, SmartIO will attempt to minimize the distance a device or a \gls{cpu} needs to read across.
        %
        The performance experiment presented in \cref{sec:eval-nvme} demonstrates this.


    \item Our performance experiments in \cref{srmpds,cc,tocs} also revealed that an \gls{iommu} in the data path can negatively affect \gls{dma} performance, as the \gls{iommu} may split large \gls{pcie} transactions into several, smaller-sized transactions.
        %
        This is especially an issue for our \gls{mdev} sharing method, as SmartIO uses the \gls{lender}'s \gls{iommu} in order to map the device to the same \gls{guestphys} address space as the \gls{vm} the device is \lgls{passthrough}{passed-through} to.
        %
        For \gls{dl}, the use of an \gls{iommu} on the \gls{lender} is optional.
        %
        However, the use of an \gls{iommu} on the \gls{borrower} is necessary (except in a few scenarios where it is possible to map the entire \gls{ram} of the \gls{borrower}). 
        %
        Consequently, this may introduce limitations on scenarios where machines act as both \glspl{lender} and \glspl{borrower}, where maximizing \gls{dma} performance is a requirement.
        %
        In the case where device drivers are implemented using the \gls{apiext}, an \gls{iommu} is entirely optional on \emph{both} the \gls{lender} and the \gls{borrower}.
\end{itemize}
%
By making it possible for remote resources to be accessed over native \gls{pcie}, \cref*{obj:performance} is solved.
%
Improving performance issues involving \glspl{iommu} is a candidate for future work.



\objdynamic*%
%
%\Glspl{lender} may even forcefully reclaim their devices, should it be necessary.
%
%
Using SmartIO, resources may be shared without requiring machines to be rebooted.
%
Devices registered with SmartIO can be borrowed by any machine, at any time, using any of the three sharing methods.
%
For example, a machine may borrow a device using \gls{dl} and at the same time run a \gls{vm} that is borrowing another device using \gls{mdev}.
%
The different sharing methods can also be combined, as demonstrated by the proof-of-concept NVMe driver experiment presented in \cref{sec:eval-nvme}.
%
When the device is no longer needed, it can be returned so it may be used by another \gls{borrower}.
%
Through borrowing and returning devices, systems may dynamically scale \gls{io} resources up or down based on current workload demands.



Devices are logically decoupled from the machines they are physically installed in, allowing software to be moved to any machine in the cluster.
%
SmartIO keeps track of both \glspl{memorysegment} and devices, and is able to locate resources in the cluster, without requiring that the user knows anything about the underlying \gls{pcie} topology.
%
The shortest path between devices, \glspl{cpu}, and \glspl{memorysegment} is determined automatically, and SmartIO configures \glspl{ntb} along that path in order to map remote memory resources for \glspl{cpu} and devices.
%
Moreover, SmartIO also supports borrowing devices from multiple \glspl{lender} and enabling \gls{p2pdma} transfers between them, as we explain in \cref{srmpds,cc,tocs}.
%
\Gls{p2p} can be enabled when borrowing devices using \gls{dl} or \gls{mdev}, which is demonstrated in the various \gls{p2p} experiments presented in these papers.
%
\Gls{p2p} is also supported when using the \gls{apiext}, which we demonstrate in the proof-of-concept \gls{nvme} driver experiment (\cref{sec:eval-nvme}).
%
Our various performance experiments demonstrate that SmartIO is a dynamic and flexible sharing framework, thus solving \cref*{obj:dynamic}.




\objdisaggregation*%
%
SmartIO is able to \gls{disaggregate} multi-function devices, such as devices capable of \gls{sriov}, and distribute individual \glspl{devicefunction} to different \glspl{borrower}.
%
An experiment demonstrating this is presented in \cref{tocs}.
%
Devices that do not support \gls{sriov} may be \gls{disaggregated} in \emph{software} instead, using our \gls{exttosisciapi}.
%
Using the \gls{apiext}, a device be borrowed by several machines simultaneously.
%
Our proof-of-concept \gls{nvme} driver presented in \cref{tocs} demonstrate this, where several \glspl{borrower} share the same (non-\gls{sriov}) \gls{nvme}.
%
In other words, the \gls{apiext} enables ``\gls{mriov} in software''.



The \gls{apiext} makes it possible to implement device drivers as part of distributed, shared-memory cluster applications. 
%
Any \gls{memorysegment} anywhere in the cluster can be mapped for devices, so they may access them directly, including \glspl{segment} in local \gls{ram} on the \gls{borrower}, \glspl{segment} in \gls{ram} on the \gls{lender}, and even \glspl{segment} in memory of a different cluster machine altogether.
%
\Glspl{devicebar} are also automatically exported by SmartIO as \glspl{sharedsegment}, allowing device memory to be mapped for the application process or even for \emph{other devices} (thus enabling \gls{p2p}).
%
As such, SmartIO supports \gls{disaggregating} device memory.
%
It is even possible to map \gls{multicasting} \glspl{segment} for a device, allowing a device to stream data to multiple destinations in a single operation.
%
Moreover, SmartIO makes it possible to associate \glspl{memorysegment} with a device (rather than a machine in the cluster), allowing the location of \glspl{memorysegment} to be abstracted away in a similar fashion to devices.
%
This allows software to be written as if all resources are local, and can run on any machine in the cluster.
%
%Not only does this allow software to be moved to any machine in the cluster, but the implementation of device drivers becomes easier as well as they can be written as if all resources are local.
%
SmartIO is able to optimize memory locations without requiring that the user is aware of the underlying \gls{pcie} network topology.
%
The proof-of-concept \gls{nvme} driver experiment presented in \cref{sec:eval-nvme} demonstrates all of these capabilities, proving that \cref*{obj:disaggregation} is solved.



\objexperiments*%
%
To prove that SmartIO is an efficient solution for real-world applications, we have performed a comprehensive performance evaluation consisting of both synthetic microbencmarks and realistic, large-scale workloads.
%
All parts of our SmartIO implementation have been evaluated, and we have included several sharing scenarios and network topologies have been evaluated, as well as a wide range of standard and commodity \gls{pcie} devices like \glspl{gpu}, \glspl{nic}, and \glspl{nvme}.
%
As SmartIO makes it possible to use remote devices as if they were local, we have used standard and unmodified benchmarking tools and device drivers.
%
Through comparison testing, we prove that SmartIO does not add any performance overhead compared to using local resources, in terms of latency and throughput.
%
We have also explored the performance effects of moving resources further away, and present a thourough analysis of this.
%
Finally, a proof-of-concept \gls{nvme} driver was developed in order to evaluate our \gls{apiext} and the \gls{disaggregation} possibilities enabled by SmartIO.
%
All of the published papers contributed towards solving this objective (\crefrange{nossdav}{tocs}).




% Refer back to the main research question
% How does this move the world forward?
By solving all of our six research objectives, we have answered the central research question of this dissertation:
\researchquestion*%
%%
We have not just implemented yet another \gls{disaggregation} solution, but developed a new and more flexible solution by taking a novel approach:
%
utilizing the memory mapping capabilities of \glspl{ntb} to unify traditional device \gls{io} with distributed, shared-memory computing.
%
Our implementation makes it possible for machines to share their inner devices and memory with other machines in a \gls{pcie} cluster.
%
Remote resources can be used over standard \gls{pcie}, making our SmartIO framework a zero-overhead solution for scaling out and transparently using more hardware resources than there are available in a single machine.
%
By lending out their own local resources and borrowing remote resources, machines take part in a dynamic and composable \gls{io} infrastructure.
%Through a process of borrowing and returning devices, machines take part in a dynamic and composable \gls{io} infrastructure.
%
Thus, we have shown that we can leverage \glspl{ntb} to develop a solution where resources can be freely shared in a \gls{pcie}-networked cluster.



\section{Future work}\label{sec:fw}
%
Several ideas for improvements emerged during the development of our SmartIO framework.
%
Here, we highlight some areas that warrant further investigations and outline some possible directions for both ongoing and future work:
%
\begin{itemize}
    \item The security implications of allowing remote machines to use and control internal system resources is something that should be addressed.
        %
        By lending away local devices, the \gls{lender} effectively yields control over it to software running on a remote system.
        %
        A flawed device driver may cause a device to read from and write to rogue memory addresses, potentially crashing the system.
        %
        A malicious driver may even \emph{intentionally} overwrite memory, or misuse \gls{dma} in order to snoop data from memory on the \gls{lender}.
        %
        This is particularly a concern in the context of our \gls{sisciapiext}, as this exposes device functionality to \gls{userspace} software.
        %
        Using an \gls{iommu} on the \gls{lender} offers some protection against undesired memory accesses, as devices are isolated in their own virtual address space.
        %
        However, as \gls{iommu} domains only isolate \emph{per device}, it could still be possible for a malicious program to interfere while others are using a device, and something like \gls{pasid} may be required.
        %
        The security challenges of one-sided initiated \gls{io} is an understudied topic in general~\cite{Shinyeh2019}, so any work in this area would likely be a significant contribution.

    
    \item Our performance experiments presented in \cref{srmpds,cc,tocs} show that a combination of using the \gls{lender}-side \gls{iommu} and long \gls{pcie} paths may severely impair \gls{dma} performance.
        %
        While the use of an \gls{iommu} on the lender is optional for \gls{dl}, it is a requirement when using our \gls{mdev} sharing method.
        %
        Additionally, since using an \gls{iommu} on the \gls{borrower} is required by \gls{dl} in most scenarios, the \gls{iommu} will be present in machines that are \emph{both} \glspl{lender} and \glspl{borrower}.
        %
        Reducing or eliminating \gls{iommu} performance penalty is a strong candidate for future improvements.
        %
        Alternative \gls{cpu}/\gls{iommu} architectures should be investigated, to determine if they behave similarly to the Intel~Xeon \glspl{cpu} used in our experiments.
        %
        Implementing support for \gls{ats}~\cite{spec:ATS} should also be considered, as \gls{ats} allows devices (and \gls{pcie} switch chips) to cache \gls{io}~addresses resolved by an \gls{iommu}.
        %
        However, it should be mentioned that since \gls{ats} requires support in both devices and \glspl{iommu}, it appears to not be widely adopted, especially for commodity hardware.


    \item Our \gls{mdev} implementation currently supports so-called ``cold migration''.
        %
        \Glspl{vm} can shut down, migrate, and restart on a different \gls{host}, while keeping the same \lgls{passthrough}{passed-through} devices.
        %
        %\Gls{vm} are assigned device identifiers in their configuration, and devices are borrowed and returned on boot and shutdown. 
        %
        If the \gls{vmemulator} supports it, it could also be possible to support \gls{hotadding} and hot-removing devices to a running \gls{vm}, making live migration theoretically possible by first removing all devices, migrating, and then re-attaching them afterwards.
        %
        However, this would temporarily disrupt device \gls{io} and force guest drivers to reset all devices.
        %
        Supporting real ``hot migration'', remapping devices while they are in use, with minimal disruption, is something we wish to implement in future work.
        %
        Not only would such a solution require keeping memory consistent during the migration warm-up, but a solution would also need to consider \gls{dma} transactions in-flight during the migration.
        %
        A mechanism for re-routing transactions, without violating the strict ordering required by \gls{pcie}, should be implemented, and would most likely require hardware support that does not exist today.


    \item While our proof-of-concept \gls{nvme} driver demonstrates that it is possible for multiple machines to share the same \gls{nvme} simultaneously, it is not very practicable by itself; 
        %
        as our implementation only provides block-level access to \gls{userspace} cluster applications, implementing a file system or coordinating access is currently the responsibility of the application.
        %
        Therefore, we are currently working on implementing our proof-of-concept prototype as a \emph{\gls{kernelspace}} driver, making the \gls{disaggregated} \gls{nvme} available to the system for general use.
        %
        This way, the \gls{disaggregated} \gls{nvme} can be used as a system disk, making it possible to format it with existing Linux file systems.
        %
        Moreover, since the \gls{nvme} can be shared with multiple machines, it would also be possible to use a shared-disk file system, such as GFS2.
        %
        This new \gls{kernelspace} implementation could  co-exist with the existing \gls{userspace} implementation, as queues can be assigned to application processes and kernel modules alike.

\end{itemize}
%
Finally, future interconnection technologies that are built on \gls{pcie} could provide new opportunities, and should be explored once they become widely available.
%
Particularly \gls{cxl}~2.0~\cite{url:CXL} is interesting in the context of memory \gls{disaggregation}, as it provides new cache-coherent protocols for accessing system and device memory.



Our critical review of the goals and objectives showed that such sharing of resources across cluster machines is possible.
%
However, there also are unsolved questions, and we have presented some additional potential directions. 
%
Nevertheless, we believe that our research results are a step in the right direction and should be a sound foundation for further research activities.
