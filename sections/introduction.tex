\chapter{Introduction}\label{sec:intro}
\section{Background and motivation}

%Modern cluster computing applications, particularly those that involve machine learning and other data-driven tasks, have high demands to \gls{io} performance. These applications may rely on a wide variety combinations of shared memory, high-volume storage, and hardware accelerators, such as \glspl{gpu} or \glspl{fpga}. 

%If \gls{io} resources, such as memory and devices, are distributed scarcely in the cluster, cluster nodes may become bottlenecks when a workload require heavy computation on \glspl{gpu} or fast access to storage. Contrarily, over-provisioning nodes with resources may lead to individual devices becoming underutilized if the workload's demands are more sporadic. In order to meet latency and throughput requirements of these workloads, and at the same time prevent that resources become underutilized, it is desirable to allow individual machines in the cluster to dynamically scale up or down their available \gls{io} resources based on current workload requirements.


%However, while moving data efficiently between networked nodes in a cluster has been a research challenge for decades, moving workloads and data to remote units over the network remains a costly operation that introduces large performance overheads compared to accessing local resources. 


%Consequently, \glspl{ntb} could be exploited to create a network where the inner components of all computers in the cluster,~i.e., \glspl{cpu} and \gls{pcie} devices, are connected to the same, shared \gls{pcie} fabric.
%the latency and throughput requirements of modern, data-driven workloads, 
%
    %that makes it easier for nodes to scale out in order to increase overall performance in the cluster and 
    
    %seamlessly combines distributed shared-memory functionality with traditional \gls{io}, has native \gls{pcie} performance, and simultaneously makes it easier to scale out and increase overall performance in the cluster system?

\cite{Taherkordi2018}
%\gls{ntb}
%\gls{pcie}
%\gls{cpu}
%\gls{fpga}
%\gls{gpu}
%\gls{dma}
%\gls{os}
%\glspl{os}
%\gls{userspace}
%\gls{sriov}

%Due to its very low latency overhead and memory addressing properties, using \gls{pcie} as a high-speed interconnection technology is a compelling alternative to traditional networking technologies. 
%
%Because \gls{pcie} was originally designed as a local \gls{io} bus, connecting devices to the \gls{cpu} on a motherboard, 
%individual computer systems operate with different \gls{pcie} address domains.
%
%Interconnecting systems using \gls{pcie} require translating memory transactions from one \gls{pcie} address domain to another. 
%
%Such address translation is made possible by using \glspl{ntb}. 
%
%By using \glspl{ntb}, a computer system can map (parts of) the address space of other, remote computer systems. 

%The \gls{ntb} translate addresses between the different address domains of the independent systems.
%

By using \glspl{ntb}, the \gls{pcie} bus of independent computers can be interconnected, creating a network fabric where the \glspl{cpu} and internal \gls{pcie} devices of all systems are directly attached.
%
Through such \gls{pcie} networking, scaling out and using more hardware resources than there are available in a single computer becomes possible, increasing the overall resource utilization and system performance. 
%
Remote resources, such as memory and devices, could be mapped into a local system and accessed through the \gls{ntb}. 
%
Similarly, a remote device capable of \gls{dma} could also use the \gls{ntb} to access local resources. 
%
However, setting up such \gls{ntb} mappings requires awareness of the address space on the remote system.
%
Device drivers interacting with a remote device must use addresses that corresponds with the remote device's address space.
%
As this greatly increases the programming complexity and require extensive modifications to existing device driver software, it is not a viable approach for enabling resource sharing among computers  as is.



\section{Problem statement}\label{sec:objectives}
Utilizing \gls{pcie} \glspl{ntb} to share resources among machines in a \gls{pcie}-networked cluster requires a 
solution for abstracting away the physical location of a resource, including the address space of the computer system it is installed in. Moreover, as it is desirable to avoid modifications to existing device drivers and application software, such a solution must also be able to present the resource to the system as if it was locally installed.
%
Hence, the goal of this thesis is to develop an infrastructure for sharing and distributing \gls{io} resources,~i.e., devices and memory, in a way that eliminates the distinction between local and remote access of a resource. The challenges of this goal are addressed under the following research question: 
\begin{quote}\itshape
    % TODO: Consider making this more general for PCIe networks instead, and remove reference to NTBs
    %       From the objectives, it should be clear that NTBs are necessary anyway
    Can \glspl{ntb} be leveraged to allow the internal memory and devices of individual computers in a \gls{pcie}-networked cluster to be shared with and used by remote machines in the cluster, as if these resources were local to the remote machines?
\end{quote}
%
In particular, this research question can be broken down into the following objectives:
%
\begin{description}
    \item[\objective{distributed}:] Ubiquitous sharing in the cluster should be supported, allowing any machine to contribute any of its internal \gls{pcie} devices, and allowing any machine to be able to use shared devices, even contributing and using devices at the same time.
\end{description}
Perhaps the main motivation for our goal of building a system for \gls{io} resource sharing is to make it easier to scale out and use more resources than there are available in a single computer. 
If any standard \gls{pcie} device inside any machine could be shared with other machines, the \gls{io} resource utilization in the cluster can be greatly increased.
Additionally, by avoiding dedicated servers and allowing all computers in the cluster to participate in the sharing, contributing their own resources and using resources shared by others, we would effectively enable a distributed, peer-to-peer sharing model. This objective sets our goal apart from existing \gls{pcie}-based solutions, as these require a central server or that devices are directly attached to a \gls{pcie} switch.

\begin{description}
    \item[\objective{transparent}:] The fact that resources may be remote should be functionally transparent, allowing systems to use remote resources in the same way as if they were local, without requiring any modifications to device hardware, device drivers, host \gls{os}, or application software.
\end{description}
If the solution could make remote devices behave as if they were locally installed, presenting resources to the system on a level ``underneath'' the \gls{os}, it would become possible to distribute devices to \emph{physical} hosts as well, and not only \glspl{vm}. 
In other words, remote resources should appear as if they were part of the local \gls{pcie} device tree, and application software could make use of remote devices using native interfaces the same way it would use local devices.
%
Furthermore, by avoiding application- or device-specific middleware, and instead memory-mapping remote system and device memory directly, existing device drivers and even the host \gls{os} itself would be able to interact with remote resources natively.
Avoiding any special adaptions to software would make scaling out significantly easier than what is currently possible with existing middleware-based solutions for distributed \gls{io}, particularly those based on \gls{rdma}.

\begin{description}    
    \item[\objective{performance}:] The fact that resources may be remote should be transparent with regard to performance, remote resources should be used with native \gls{pcie} performance, and as close to local access as possible.
\end{description}
Moving data efficiently between networked machines in a cluster has been a research challenge for decades. Despite this, moving workloads and data to remote units over the network remains a costly operation that introduces large performance overheads compared to accessing local resources. 
Therefore, in order to further blur the hard separation between ``remote'' and ``local'', remote resources should not only behave functionally as if they were locally installed in the system using them, but also have comparable performance.
To achieve this, any communication overhead and intermediate data copying in the critical path must be completely avoided, a requirement that rules out (most) traditional methods of sharing resources over a network. 
Remote resources should be accessed directly over \emph{native} \gls{pcie}, which would improve the overall \gls{io} performance in the cluster.

\begin{description}    
    \item[\objective{dynamic}:] Shared resources should be distributed \emph{dynamically}, and direct access to device memory and system memory should be configured in run-time, also between multiple devices residing in different hosts.
\end{description}
As stated in \cref{obj:transparent}, the solution should work for physical hosts, and not only \glspl{vm}. Therefore, it must be possible to assign and reassign resources while all machines in the cluster are running, without requiring rebooting hosts or changing settings in the \gls{bios}.
For devices, this introduces the requirement that the \gls{os} supports hot-adding devices to the system --- something most modern \gls{os} implementations do.
Not only would this would allow systems to dynamically scale up or down their \gls{io} resources based on immediate workload requirements, but devices could be more efficiently partitioned between machines in the cluster, increasing the overall resource utilization. 
Furthermore, the solution should also be able to automatically discover resource location, without requiring that the user knows anything about the underlying \gls{pcie} network topology, and dynamically set up memory-mappings between devices, \glspl{cpu}, and memory resources. An example would be enabling \gls{pcie} \gls{p2p} between two or more \gls{dma}-capable devices that are physically installed in different machines.

    
\begin{description}
    \item[\objective{disaggregation}:] \Gls{disaggregation} of system memory, device memory, and device \emph{functionality} should be supported, and the solution should be able to distribute component parts to different hosts, as well as provide software facilities for resources that do not support \gls{disaggregation} in hardware.
\end{description}
Because most device drivers are written in a way that assumes exclusive control over a device, some devices implement virtualization support in hardware,~i.e., \gls{sriov}, that makes them appear to a system as having multiple \glspl{vf}. 
%
The solution should be able to \glslink{disaggregation}{disaggregate} such \gls{sriov}-capable devices, and distribute their \glspl{vf} to different machines, allowing multiple computers to use the same device simultaneously.
%
However, since not all devices implement \gls{sriov}, the solution should also provide a device driver \gls{api} that will make it possible to \glslink{disaggregation}{disaggregate} memory and device resources in software.
%
In addition to the native sharing capabilities described in \crefrange{obj:distributed}{obj:dynamic}, this \gls{api} 
would provide facilities for memory-mapping device registers as well as mapping shared memory segments for a \gls{dma}-capable device.
%
Effectively, this would bring shared-memory concepts to device driver implementations, allowing device operation and device resources to become part of the same global address space as distributed cluster applications.
%
This would allow multiple machines to simultaneously share the same, non-\gls{sriov} device, as well as making it possible to combine traditional \gls{io} with \gls{pcie} cluster capabilities such as zero-copy data transfer and multicasting.
%
Moreover, the \gls{api} should be designed so that a driver implementation does not need to consider the system-local address space of the computer system where a device is installed, thus alleviating the complexity of programming device drivers for remote devices using \glspl{ntb}.

\begin{description}    
    \item[\objective{experiments}:] To prove real-world deployment capabilities, the solution should be tested on realistic and relevant workloads and benchmarks.
\end{description}
To confirm that \gls{io} resources can be distributed to, and shared with, remote machines, a comprehensive performance evaluation covering all components of the implementation is needed.
As the solution should provide \emph{native} \gls{pcie} performance (\cref{obj:performance}), all parts should be thoroughly tested with latency and throughput in mind, in order to reveal any potential performance bottlenecks.
Standardized test suites should be used as far as possible, to prove that application software really can be unmodified (\cref{obj:transparent}).
%
Moreover, to demonstrate that the implementation is a viable solution for scaling resources (\crefrange{obj:distributed}{obj:transparent}), the evaluation should also include workloads relying on different \gls{pcie} network topologies and include several types of devices, such as \glspl{nvme}, \glspl{gpu}, and network cards
%
Finally, a prototype device driver using the device-oriented \gls{api} (\cref{obj:disaggregation}) should be developed and evaluated. This driver should demonstrate that it is possible to \glslink{disaggregation}{disaggregate} a non-\gls{sriov} device in software, and shared with multiple machines simultaneously. The driver should also demonstrate how it can rely on memory \gls{disaggregation} and shared memory capabilities to implement data path optimizations.


\section{Scope and limitations}
% limitation: systems must support pcie p2p (or use switches that do)
%limitation: systems must support hot-plugging
%not disaggregated os, not general networks, not 50,000 nodes etc
% outside scope: orchestration, algorithms for sharing, fairness, etc
% scope
% both physical hosts and virtual hosts
% also shared memory applications
% distributed, unlike existing solutions, true peer-to-peer sharing
% commodity servers, multi platform amd, intel, arm

% performance, how much performance overhead in using remote devices?

% limitations
% safety, security (iommu, encryption)
% must use ntbs (dolphin ntbs), only sisci
% not a finished product (no orchistration sw [yet])
% for linux only?

\section{Research methodology}

\section{Contributions}
The work of this thesis contributes to the topic of \gls{io} facilitation and resource sharing in distributed computing systems, and has been presented in five peer-reviewed venues: two conference workshop publications, one short-length demonstration paper, and two journal articles.
These publications are included as \crefrange{paper:nossdav}{paper:tocs} and contain the bulk of the implementation details, particularly \cref{paper:tocs} which presents the entire solution as a whole.

In short, we have developed a system called \emph{SmartIO} for sharing resources and distributing devices in a heterogeneous, \gls{pcie}-networked cluster. 
%
In particular, the main contributions of this thesis are listed as follows:
\begin{itemize}
    \item Testing and evaluation of a mechanism for distributing \gls{pcie} devices to remote systems (see \cref{paper:nossdav} and \cref{paper:mmsys}). \Glspl{nvme}, \glspl{gpu}, network adapters, and any standard \gls{pcie} device may be assigned to a remote system, appearing to the remote system as if the device has been dynamically hot-added to the system.

	\item Development of an extension to \gls{kvm} based on the \gls{mdev} interface, implementing support for distributing devices to \glspl{vm} running on any host machine in the cluster (see \cref{paper:srmpds} and \cref{paper:cc}). Direct access to remote physical hardware devices are enabled for \gls{vm} guests. This allows (remote) device resources to be ``passed through'' to \glspl{vm} and used with bare-metal performance.
	
    \item Improvement of the device distribution mechanism, and the \gls{mdev} extension of it, with support for devices in different physical machines (see \cref{paper:srmpds} and \cref{paper:cc}). A method for resolving device memory addresses and setting up memory mappings, in a way that is transparent to both the devices and the device drivers, is implemented. This enables direct data transfers between multiple devices without violating the principle of making devices appear local to the system(s) using them.

    \item Development of a new device-oriented \gls{api} for writing device drivers as shared-memory applications (see \cref{paper:tocs}).
    %
	This \gls{api} makes it possible to \glslink{disaggregation}{disaggregate} devices and device memory in software, similarly to \gls{rdma} \gls{disaggregation} solutions.
	Unlike \gls{rdma} solutions, however, remote resources can be memory-mapped directly into the virtual address space of a software process.
	%
	Through our \gls{api}, device driver implementations may take full advantage of \gls{pcie}
	shared memory capabilities, such as remote memory access and multicasting, without requiring
	awareness of the underlying \gls{pcie} topology and the different address domains of remote systems.
	%
	This makes it easier to optimize data flow through the \gls{pcie} network, as software no longer needs to be written with accessing remote resources in mind, but can be implemented as if resources are local.

    \item Development of a prototype \gls{nvme} device driver using our device-oriented \gls{api} (see \cref{paper:tocs}).
    Although the device distribution components of SmartIO makes it possible to use existing device drivers, most device drivers are written in a way that assumes exclusive control over the device. Therefore, a distributed device (function) may only be used by a single user at the time.
    To demonstrate software-enabled \gls{disaggregation}, we have implemented a \emph{distributed} \gls{nvme} driver. As a proof of concept, we show a single NVMe device can be shared and operated by multiple cluster machines simultaneously, without requiring \gls{sriov}.
	This driver also demonstrates how multiple sharing aspects of SmartIO may be combined, 
	by \glslink{disaggregation}{disaggregating} remote GPU memory and enabling memory access optimizations.

    \item A comprehensive performance evaluation covering all parts of SmartIO and the implementation of performance optimizations (see \cref{paper:mmsys}, \cref{paper:srmpds}, \cref{paper:cc}, and \cref{paper:tocs}). With the goal of not incurring any performance overhead beyond that of native \gls{pcie}, the performance of using remote resources with SmartIO is comparable to that of local access (in terms of latency and throughput).
    To demonstrate real-world capabilities, two different image classification workloads relying on multiple \glspl{gpu} and \gls{nvme} storage have been tested with SmartIO, in order to prove that it is a viable solution for cost-effective scaling of \gls{io} resources in realistic scenarios.
	
\end{itemize}


Finally, it should be mentioned that several components of SmartIO have already been incorporated into the product line of Dolphin Interconnect Solutions, and others are currently being adapted for real-world deployment.


%\begin{itemize}
%
%	\item We have incorporated and improved 
%
%		We have incorporated our previous Device~Lending method into our complete SmartIO solution.
%		NVMes, GPUs, network adapters, and any standard PCIe device can be distributed to remote systems
%		and used without any performance difference compared to local access.
%		%
%		Devices appear as if they are dynamically hot-added to the system, and can be used by 
%		existing application software and device drivers without requiring any modifications.
%
%	\item SmartIO also includes our MDEV extension to Device~Lending. This interface extends the Linux
%		Kernel-based Virtual Machine hypervisor~(KVM).
%		%
%		
%		

%
%	\item We have developed a prototype NVMe device driver using our new device-oriented API. Although the Device~Lending
%		component of SmartIO makes it possible to use existing device drivers, most device drivers
%		are written in a way that assumes exclusive control over the device. Using Device~Lending
%		alone, a device may only be used by a single host at the time.
%		%
%		To demonstrate software-enabled disaggregation, we have implemented a \emph{distributed} NVMe
%		driver.
%		%
%		As a proof of concept, we show a single NVMe device can be shared and operated by $30$
%		cluster nodes simultaneously, without requiring SR-IOV.
%		%
%		This driver also demonstrates how multiple sharing aspects of our system may be combined, 
%		by disaggregating (remote) GPU memory and enabling memory access optimizations.
%
%%		Furthermore, by using our NVMe driver to access memory of (remote) GPUs that are managed
%%		by the standard Nvidia GPU driver, we show the completeness of our sharing system through
%%		combining Device~Lending and our new API.
%
%%	\item fully distributed, all nodes contribute, scale out, sharing on different abstraction levels
%%		As any node in the cluster may contribute their own devices and access remote resources, even
%%		at the same time, the hard distrinction between remote and local resources become blurred
%%		out, making it easier to scale out and increase overall I/O resource utilization in the
%%		system.
%
%	\item To prove that our solution enables zero-overhead sharing, we provide a comprehensive 
%		performance evaluation covering all components of our SmartIO solution,
%		including our earlier Device~Lending and MDEV work. We have performed entirely new
%		experiments, using both synthetic microbenchmarks and realistic large-scale workloads.
%		%
%		Our experimental results confirm that I/O devices can be distributed to, and shared with,
%		remote hosts, without any performance penalty beyond what is expected for longer PCIe paths. In fact, all our
%		experiments prove that remote devices can be used \emph{without any performance overhead}
%		compared to local access in terms of latency and throughput.
%
%
%\end{itemize}


%

\section{Outline}
%outline also of papers, to hammer home that this is a paper collection
