\chapter{Introduction}\label{sec:intro}
\section{Background and motivation}
%Modern cluster computing applications, particularly those that involve machine learning and other data-driven tasks, have high demands to \gls{io} performance. These applications may rely on a wide variety combinations of shared memory, high-volume storage, and hardware accelerators, such as \glspl{gpu} or \glspl{fpga}. 

%If \gls{io} resources, such as memory and devices, are distributed scarcely in the cluster, cluster nodes may become bottlenecks when a workload require heavy computation on \glspl{gpu} or fast access to storage. Contrarily, over-provisioning nodes with resources may lead to individual devices becoming underutilized if the workload's demands are more sporadic. In order to meet latency and throughput requirements of these workloads, and at the same time prevent that resources become underutilized, it is desirable to allow individual machines in the cluster to dynamically scale up or down their available \gls{io} resources based on current workload requirements.


%However, while moving data efficiently between networked nodes in a cluster has been a research challenge for decades, moving workloads and data to remote units over the network remains a costly operation that introduces large performance overheads compared to accessing local resources. 


%Consequently, \glspl{ntb} could be exploited to create a network where the inner components of all computers in the cluster,~i.e., \glspl{cpu} and \gls{pcie} devices, are connected to the same, shared \gls{pcie} fabric.
%

\cite{Taherkordi2018}
%\gls{ntb}
%\gls{pcie}
%\gls{cpu}
%\gls{fpga}
%\gls{gpu}
%\gls{dma}
%\gls{os}
%\glspl{os}
%\gls{userspace}
%\gls{sriov}

%Due to its very low latency overhead and memory addressing properties, using \gls{pcie} as a high-speed interconnection technology is a compelling alternative to traditional networking technologies. 
%
%Because \gls{pcie} was originally designed as a local \gls{io} bus, connecting devices to the \gls{cpu} on a motherboard, 
%individual computer systems operate with different \gls{pcie} address domains.
%
%Interconnecting systems using \gls{pcie} require translating memory transactions from one \gls{pcie} address domain to another. 
%
%Such address translation is made possible by using \glspl{ntb}. 
%
%By using \glspl{ntb}, a computer system can map (parts of) the address space of other, remote computer systems. 

%The \gls{ntb} translate addresses between the different address domains of the independent systems.
%

By using \glspl{ntb}, the \gls{pcie} bus of independent computers can be interconnected, creating a network fabric where the \glspl{cpu} and internal \gls{pcie} devices of all systems are directly attached.
%
Through such \gls{pcie} networking, scaling out and using more hardware resources than there are available in a single computer becomes possible, increasing the overall resource utilization and system performance. 
%
Remote resources, such as memory and devices, could be mapped into a local system and accessed through the \gls{ntb}. 
%
Similarly, a remote device capable of \gls{dma} could also use the \gls{ntb} to access local resources. 
%
However, setting up such \gls{ntb} mappings requires awareness of the address space on the remote system.
%
Device drivers interacting with a remote device must use addresses that corresponds with the remote device's address space.
%
As this greatly increases the programming complexity and require extensive modifications to existing device driver software, it is not a viable approach for enabling resource sharing among computers  as is.



\section{Problem statement}\label{sec:objectives}
Utilizing \gls{pcie} \glspl{ntb} to share resources among machines in a \gls{pcie}-networked cluster requires a 
solution for abstracting away the physical location of a resource, including the address space of the computer system it is installed in. Moreover, as it is desirable to avoid modifications to existing device drivers and application software, such a solution must also be able to present the resource to the system as if it was locally installed.
%
Hence, the goal of this thesis is to develop an infrastructure for sharing and distributing \gls{io} resources,~i.e., devices and memory, in a way that eliminates the distinction between local and remote access of a resource. The challenges of this goal are addressed under the following research question: 
\begin{quote}\itshape
    % TODO: Consider making this more general for PCIe networks instead, and remove reference to NTBs
    %       From the objectives, it should be clear that NTBs are necessary anyway
    Can \glspl{ntb} be leveraged to allow the internal memory and devices of individual computers in a \gls{pcie}-networked cluster to be shared with and used by remote machines in the cluster, as if these resources were local to the remote machines?
\end{quote}

In particular, this research question can be broken down into the following objectives:
%
\begin{description}
    \item[\objective{distributed}:] Ubiquitous sharing in the cluster should be supported, allowing any machine to contribute any of its internal \gls{pcie} devices, and allow any machine to be able to use shared devices, even contributing and using resources at the same time.
\end{description}
Perhaps the main motivation for our goal of building a system for \gls{io} resource sharing is to make it easier to scale out and use more resources than there are available in a single computer. 
If any standard \gls{pcie} device inside any machine could be shared with other machines, the \gls{io} resource utilization in the cluster can be greatly increased.
Additionally, by avoid dedicated resource servers and allowing all computers in the cluster to participate in the sharing, contributing their own resources and using resources shared by others, we would effectively enable a distributed, \gls{p2p} sharing model. This objective sets our solution apart from existing \gls{pcie}-based solutions, as these require a central server or that devices are directly attached to a \gls{pcie} switch.

\begin{description}
    \item[\objective{transparent}:] The fact that resources may be remote should be functionally transparent, allowing systems to use remote resources in the same way as if they were local, without requiring any modifications to device hardware, device drivers, host \gls{os}, or application software.
\end{description}
If our solution could make remote devices behave as if they were locally installed, presenting resources to the system on a level ``underneath'' the \gls{os}, it would become possible to distribute devices to \emph{physical} hosts as well, and not only \glspl{vm}. 
In other words, remote resources should appear as if they were part of the local \gls{pcie} device tree, and application software could make use of remote devices using native interfaces the same way it would use local devices.
%
Furthermore, by avoiding application- or device-specific middleware, and instead memory-mapping remote system and device memory directly, existing device drivers and even the host \gls{os} itself would be able to interact with remote resources natively.
Avoiding any special adaptions to software would make scaling out significantly easier than what is currently possible with existing middleware-based solutions for distributed \gls{io}, particularly those based on \gls{rdma}.

\begin{description}    
    \item[\objective{performance}:] The fact that resources may be remote should be transparent with regard to performance, remote resources should be used with native \gls{pcie} performance, and as close to local access as possible.
\end{description}
Moving data efficiently between networked machines in a cluster has been a research challenge for decades. However, moving workloads and data to remote units over the network remains a costly operation that introduces large performance overheads compared to accessing local resources. 
Therefore, in order to further blur the hard separation between ``remote'' and ``local'', remote resources should not only behave functionally as if they were locally installed in the system using them, but also have comparable performance.
To achieve this, any communication overhead and intermediate data copying in the critical path must be completely avoided, a requirement that rules out (most) traditional methods of sharing resources over a network. 
Remote resources should be accessed directly over \emph{native} \gls{pcie}, which would improve the overall \gls{io} performance in the cluster.

\begin{description}    
    \item[\objective{dynamic}:] Shared resources should be distributed \emph{dynamically}, and direct access to device memory and system memory should be configured in run-time, also between multiple devices residing in different hosts.
\end{description}
As stated in \cref{obj:transparent}, the solution should work for physical hosts, and not only \glspl{vm}. Therefore, it must be possible to assign and reassign resources while all machines in the cluster are running, without requiring that hosts reboot or changes to settings in the \gls{bios}.
For devices, this introduces the requirement that the \gls{os} supports hot-adding devices to the system --- something most modern \gls{os} implementations do.
Not only would this would allow systems to dynamically scale up or down their \gls{io} resources based on immediate workload requirements, but devices could be more efficiently partitioned between machines in the cluster, increasing the overall resource utilization. 
Furthermore, the solution should also be able to automatically discover resource location, without requiring that the user knows anything about the underlying \gls{pcie} network topology, and dynamically set up memory-mappings between devices, \glspl{cpu}, and memory resources. An example would be enabling \gls{pcie} \gls{p2p} between two or more \gls{dma}-capable devices that are physically installed in different machines.

    
\begin{description}
    \item[\objective{disaggregation}:] \Gls{disaggregation} of system memory, device memory, and device \emph{functionality} should be supported, and the solution should be able to distribute component parts to different hosts, as well as provide software facilities for resources that do not support \gls{disaggregation} in hardware.
\end{description}


Some details here (combining traditional I/O with shared memory, machine that lends out still is operative), also nvme driver here? , we would be able to support sharing on multiple abstraction levels. sriov, software \gls{api}


\begin{description}    
    \item[\objective{experiments}:] To prove real-world deployment capabilities, the solution should be tested on realistic and relevant workloads and benchmarks, and an exhaustive performance evaluation of all components of the implementation should be provided.
 \end{description}
including benchmarks for a bunch of scenarios and devices and topologies


%Allow all hosts to contribute their own devices and access remote resources, even at the same time
%The system should allow all computers in the network to contribute their own resources, and access remote resources as if they were locally installed.
%
%Support dynamic hardware configuration that allows the operating system and user space applications to make use of remotely installed devices in the same way as locally installed devices
%Our system effectively makes all
%hosts, including their internal resources (both devices and memory), part of a common PCIe domain.
% - must be dynamic / no reboot required
% - no performance overhead caused by communication
% - scaling out and increasing overall I/O utilization / performance becomes easier
% - multiple abstr levels (physical hosts, vms, disaggregation)
% - all standard PCIe devices
% - should work as if locally installed / abstract away network topology / blur distinction between remote and local
% - "as if local" also means performance(!)
% - combines traditional distributed I/O with shared-memory mindset
% - distributed
%This would allow \gls{os} and \gls{userspace} applications to make use of remotely installed resources in the same way as if they were locally installed. 

%If computers in a cluster are to contribute their own devices and memory and share these internal resources with other machines in the cluster, it is desirable for machines to use remote resources in the same way they would use local resources.

%\begin{quote}
 %   Can we develop a framework for sharing resources in a \gls{pcie}-networked cluster that makes it easier for individual nodes to scale out and use remote resources while maintaining native \gls{pcie} performance.
%\end{quote}


    %that makes it easier for nodes to scale out in order to increase overall performance in the cluster and 
    
    %seamlessly combines distributed shared-memory functionality with traditional \gls{io}, has native \gls{pcie} performance, and simultaneously makes it easier to scale out and increase overall performance in the cluster system?
%\end{quote}    
%    minimizes complexity blurs the distinction 
    
 
%    by blurring the distinction between remote and local, 
    
 %   distributing devices and sharing resources 
    
    
	%Can we blur out the distinction between local and remote in a \gls{pcie}-cluster by developing a system for sharing and distributing resources
	%that allows nodes to share resources with native \gls{pcie} performance.

%This has created a need for an efficient \gls{io} resource sharing solution.

%	in order to meet latency and throughput requirements of modern, data-driven workloads?
%In this thesis, we address this challenge under the following research question:
%\begin{quote}\bfseries
%	Can we develop a system for sharing and distributing resources in a \gls{pcie}-networked cluster, allowing cluster nodes to share resources with native \gls{pcie} performance,	in order to meet latency and throughput requirements of modern, data-driven workloads?
%\end{quote}

%the latency and throughput requirements of modern, data-driven workloads, 


\section{Scope and limitations}
% limitation: systems must support pcie p2p (or use switches that do)
%limitation: systems must support hot-plugging
%not disaggregated os, not general networks, not 50,000 nodes etc
% outside scope: orchestration, algorithms for sharing, fairness, etc
% scope
% both physical hosts and virtual hosts
% also shared memory applications
% distributed, unlike existing solutions, true peer-to-peer sharing
% commodity servers, multi platform amd, intel, arm

% performance, how much performance overhead in using remote devices?

% limitations
% safety, security (iommu, encryption)
% must use ntbs (dolphin ntbs), only sisci
% not a finished product (no orchistration sw [yet])
% for linux only?

\section{Research methodology}

\section{Contributions}
% should experiments be central

\section{Outline}
