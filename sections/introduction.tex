\chapter{Introduction}\label{chapter:intro}

\begin{figure}
	\centering
	\includegraphics{pooling}
    \caption
    [If machines could pool their internal devices, cluster nodes could dynamically compose the \glsfmtshort{io} infrastructure by borrowing devices from other machines]
    {If machines could pool their internal devices, it would be possible to avoid queuing work on dedicated machines with particular device configurations. Instead, each machine could dynamically compose the \gls{io} infrastructure needed to meet a workload's \gls{io} requirements, by borrowing devices from other machines and releasing them when they are no longer needed.}
    \label{fig:device-pool}
\end{figure}

Cluster computing applications often have high requirements to \gls{io} performance.
%
%For example, compute accelerators, such as \glspl{gpu} and \glspl{fpga}, are often used to increase the processing speed of computing tasks.
For example, many computing clusters rely on compute accelerators, such as \glspl{gpu} and \glspl{fpga}, to increase the processing speed.
%
In recent years, we have also seen a convergence of the high-performance computing, big~data, and machine~learning research fields.
%
This development has led to new demands to \gls{io} performance where fast access to high-volume storage devices is becoming a requirement for high-performance computing, while low latency networking and making use of compute accelerators have become cloud computing issues~\cite{Trivedi2011,Coates2013,Taherkordi2018}.
%
If \gls{io} resources~(devices) are scarcely distributed in the cluster, cluster machines with \gls{io} resources may become bottlenecks, for example when a workload requires heavy computation on \glspl{gpu} or fast access to storage.
%
Contrarily, over-provisioning machines with resources may lead to devices becoming underutilized if a workload's \gls{io} demands are more sporadic.
%
Distributed processing workloads may even require a heterogeneous cluster design, with widely different compositions of devices and memory resources for individual machines in the cluster.
%
Being able to share and partition devices between cluster machines at run-time leads to more efficient utilization, as individual machines may dynamically scale up or down \gls{io} resources based on current workload requirements (\cref{fig:device-pool}).



In order to meet the latency and throughput requirements of data-driven and compute-heavy workloads, there is a need for flexible, yet efficient, sharing of \gls{io} resources in a networked computing cluster.
%
This dissertation contributes to this goal by presenting a solution that enables distributing devices and sharing memory resources between machines interconnected with \gls{pcie}.
%
By leveraging memory mapping functionality supported by the \gls{pcie} networking hardware, we make it possible to use resources residing in remote machines as if they were installed in the same machine.
%
Whether resources are local or remote is made transparent to application software, \gls{os}, and even device drivers, and remote resources can be used in a manner that is indistinguishable from using resources attached to the local \gls{pcie} bus.
%
Existing device drivers and application software may use remote resources without requiring any adaptations.
%
Not only does this make it easier to increase the overall resource utilization in the cluster, but it also becomes easier to design and implement distributed applications as software no longer needs to be written with accessing remote resources in mind,
but can instead be implemented as if all resources are local.
%
Using our solution, \gls{io} resources are no longer locked to individual machines, and memory and devices can be shared freely with other machines in the cluster.
%
Scaling out and using more hardware resources than there are available in a single machine becomes easier, and our solution improves both the performance of individual machines and the entire cluster as a whole.



\section{Background and motivation}\label{sec:motivation}
In cloud computing environments, dynamically scaling resources is often accomplished through virtualization. 
%
\Gls{vm} \glspl{hypervisor} may dynamically add virtual \gls{io} devices to \gls{vm} instances on demand.
%
It is even possible to temporarily suspend computation to migrate \glspl{vm} to \gls{host} machines with more hardware resources available, should the requirements of a \gls{vm} \gls{guest} exceed the available local resources.
%
However, when the raw, bare-metal \gls{io} performance is required, for example in the case of \gls{gpu}-intensive machine~learning workloads, resource virtualization may not be a viable solution.
%
In this regard, it is possible to \lgls{passthrough}{``pass through''} physical \gls{io} devices to a \gls{vm} \gls{guest} using an \gls{iommu}.
%
The \gls{iommu} facilitates direct access to hardware from the \gls{guest} without compromising the virtualized environment.
%
As such, \gls{passthrough} allows physical hardware to be used by a \gls{vm} \gls{guest} with minimal software overhead.
%
However, as physical devices are tightly coupled with the hosts they are installed in, this \gls{passthrough} technique suffers from a lack of flexibility.
%
Distributing \glspl{vm} across hosts in the network in a way that maximizes resource utilization and adapts dynamically to varying \gls{io} requirements, without sacrificing the bare-metal performance that pass-through provides, remains a challenge.



Another challenge is the networking technology itself. 
%
Despite having been a research topic for decades, moving data to remote computing units over a network remains a costly operation that introduces large performance overheads compared to using local resources.
%
As such, many \glspl{nic} support zero-copy of application memory from one system to another through \gls{rdma}~\cite{Huang2012}.
%
\Gls{rdma} is not only used in many distributed shared-memory cluster applications, but is also frequently used for implementing \gls{io} resource \gls{disaggregation} in software.
%
For example, \glspl{nvme} may be \lgls{disaggregation}{disaggregated} and shared with remote systems with very low latency.
This is the case for \gls{nvmeof}, where \gls{rdma} is used to provide direct access and avoid going through the block-layer of the \gls{os} on the server~\cite{Guz2018}.
%
Similarly, the result of a \gls{gpu} computation may be copied out of \gls{gpu} memory and onto the network directly using \gls{rdma}, without being copied to system memory first and going through the network stack~\cite{Venkatesh2014}.
%
However, while \gls{rdma} allows data to be transferred efficiently over the network, translation between the network protocol and the local \gls{io} bus is unavoidable. 
%
Compared to accessing a local device, this protocol translation incurs latency overheads that are not insignificant.
%
Moreover, as \gls{rdma} requires the use of specific programming models like message-passing~\cite{Jiang2004}, \gls{disaggregation} solutions based on \gls{rdma} are usually implemented either as application-specific \gls{middleware}, or as part of the application itself.
%
The sharing capabilities of \gls{rdma} solutions are, therefore, often limited to a single type of device.
Sharing several types of devices, for example both \glspl{gpu} and \glspl{nvme}, usually requires multiple \gls{disaggregation} implementations, and integrating them with each other may be a challenge.



\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=.85\linewidth]{fabric-partitioning}
        \caption{Partitioning allows \glsfmtshortpl{cpu} and devices with different address domains to be isolated.}
    \end{subfigure}
    \par\vspace{5mm}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=.85\linewidth]{fabric-partitioning-device-tree}
        \caption{Machines have separate (logical) \glsfmtshort{pcie} device trees.}
    \end{subfigure}
    \caption[\Glsxtrshort{pcie} switch chips with partitioning support can be used to connect multiple \glsxtrshortpl{cpu} and freestanding devices to a common \glsxtrshort{pcie} fabric.]
    {\Glsfmtshort{pcie} switch chips with partitioning support can be used to connect multiple \glsfmtshortpl{cpu} and freestanding devices to a common \glsfmtshort{pcie} fabric. However, as systems are isolated, shared memory communication over \glsfmtshort{pcie} is not possible.}
  	\label{fig:partitioning}
\end{figure}



\begin{figure}
	\centering
    \includegraphics[width=.85\linewidth]{cluster-example}
    \caption[Example of a heterogeneous \glsfmtshort{pcie}-networked cluster with \glsfmtshort{ntb} adapter cards and external cables]{Example of a heterogeneous \glsxtrshort{pcie} cluster with external \glsxtrshort{pcie} links using adapter cards capable of \glsxtrshort{ntb}. The \glsxtrshortpl{cpu} as well as internal devices of all cluster machines~(nodes) are all attached to the same \glsxtrshort{pcie} network fabric.}
  	\label{fig:cluster-example}
\end{figure}



\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics{direct-access-rdma}
        \caption{Accessing remote resources over traditional network using \glsxtrshort{rdma}.}
    \end{subfigure}
    \par\vspace{5mm}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics{direct-access-smartio}
        \caption{Accessing remote resources over native \glsxtrshort{pcie} using \glsxtrshort{ntb}.}
    \end{subfigure}
    \caption
    [Remote hardware can be accessed directly without any software in the critical path by using \glsfmtshortpl{ntb}]
    {Many distributed \gls{io} solutions have performance overheads because they rely on \gls{middleware} or other forms of software facilitation on the remote system. By setting up memory mappings through the \glsxtrshort{ntb}, remote hardware resources can be accessed directly without any software in the critical path.}
    \label{fig:direct-access}
\end{figure}



Extending the \gls{pcie} bus out of a single computer system and using it as a high-speed interconnection technology is a compelling alternative to distributed \gls{io} over a traditional network.
%
As \gls{pcie} is the standard for connecting \gls{io} devices to a local computer system, using only native \gls{pcie} would have clear performance advantage, since conversion between network protocol and \gls{io} bus would not be necessary~\cite{Fountain2005,Ravindran2008,whitepaper:Regula2004}.
%
However, since \gls{pcie} was originally designed to connect devices to the local \gls{cpu} on a motherboard, individual computer systems operate with different \gls{pcie} address domains.
%
Because of this, some \gls{pcie} switch chip hardware have virtualization support for dynamic partitioning~\cite{Chung2018,whitepaper:IDT,whitepaper:Microsemi,url:rackscale,url:liqid,url:gigaio}. 
%
Multiple \glspl{cpu} can be connected to the same \gls{pcie} fabric by mapping partitions to the individual address domains of each \gls{cpu}.
%
Additionally, devices can be attached directly to the partitionable switch chip, rather than being owned by individual machines.
%
This allows switch-attached devices to be logically assigned to different machines on the fly, as illustrated in \cref{fig:partitioning}.


Nonetheless, because partitioning isolates \glspl{cpu} in separate address domains, this approach does not make it possible for machines to share their \emph{internal} resources.
%
Memory, or other devices that are attached to the local \gls{pcie} bus and not the partitionable switch (chip), cannot be shared.
%
Thus, partitioning lacks the shared-memory capabilities needed to support host-to-host communication over native \gls{pcie}, and other networking technologies, such as Ethernet or InfiniBand, must be used instead.
%
Consequently, \lgls{disaggregation}{disaggregating} devices and sharing them with multiple machines at the same time require either alternative methods, like \gls{rdma}, or additional virtualization support in the device itself,~i.e., \gls{sriov}.
%Distributed applications must instead use other networking technologies to communicate, such as Ethernet or InfiniBand, 
%
While approaches using \gls{pcie} fabric partitioning can be said to enable a composable \gls{io} infrastructure~\cite{Chung2018}, they stop short of providing \emph{networking} capabilities over \gls{pcie}.



Due to its intrinsic memory addressing abilities and low latency overhead, it is desirable to use \gls{pcie} as the enabling networking technology for distributed, shared-memory communication~\cite{Shim2018,whitepaper:Regula2004,url:Meduri2011}.
%
This requires translating memory transactions from one machine's address domain to another.
%
By far, the most common way of translating addresses between different \gls{pcie} address domains is by using a special type of device called a \gls{ntb}~\cite{whitepaper:PLX,whitepaper:Regula2004,Hou2013,Tu2014}.
%
\Glspl{ntb} can be embedded as a \gls{cpu} feature~\cite{whitepaper:Sullivan2010,url:LinuxNTB-AMD}, but are more commonly implemented in \gls{pcie} switch chips~\cite{whitepaper:PLX,pex8733}.
%
Using such \gls{ntb}-capable switch chips to implement peripheral devices and cluster switches, it becomes possible to connect independent computer systems with plug-in host adapter cards and external cables, as depicted in \cref{fig:cluster-example}.
%
The memory address translation capabilities of \glspl{ntb} make it possible for a machine to map (parts of) the address space of remote systems.
%
Since all memory address translations are done in the \gls{ntb} hardware, memory-to-memory transfers are supported with very low latency~\cite{Lim2019,Tu2014}.



More interestingly, however, is the fact that in such \gls{ntb}-based networks, all \glspl{cpu} and internal \gls{pcie} devices are attached to the same, shared \gls{pcie} fabric.
%
Remote resources, such as the internal memory and devices of other machines, could be mapped into a local system and accessed through the \gls{ntb} with very little performance overhead.
%
Similarly, a device capable of \gls{dma} could also use the \gls{ntb} to access remote resources.
%
This approach eliminates the need to use memory on the remote system as an intermediate step when transferring data, and any software in the data transfer path can be avoided entirely, as shown in \cref{fig:direct-access}.
%
Rather than relying on dedicated servers or requiring that devices are attached to special switches, we could create distributed, peer-to-peer sharing system using \glspl{ntb};
%
\emph{all} machines in the cluster can share \emph{all} of their resources, including internal \gls{pcie} devices and system memory. 
%
Moreover, as centralized servers can be avoided, the risks of individual machines becoming performance bottlenecks or single points of failure are reduced.



However, setting up such \gls{ntb} mappings requires awareness of the address space on the remote system.
%
For example, a device driver must use addresses that correspond to the remote device's address space when initiating \gls{dma} transfers.
%
Hence, relying on \glspl{ntb} alone for enabling distributed \gls{io} and resource sharing is not a viable solution.
%
In order to manage multiple address space layouts, extensive modifications to existing device driver software would be required.
%
This is infeasible due to the vast amount of devices and device driver implementations that exist.
%
In this context, it is possible to use virtualization to mitigate the complexity of managing different \gls{pcie} address spaces.
%
The fact that devices are on the other side of an \gls{ntb} could be hidden for device drivers by distributing devices to \gls{vm} \glspl{guest} instead of physical \lgls{host}{host~machines}.
%
The \gls{hypervisor} on the host~machines can be modified to assist setting up necessary \gls{ntb} and \gls{iommu} mappings to the respective device memory and \gls{vm} memory, enabling \gls{passthrough} of remote devices to \gls{vm} \glspl{guest}~\cite{Tu2013}.
%
However, by relying on virtualization to hide complexity, this approach forgoes the possibility of using bare-metal machines for processing.
%
Even though \gls{passthrough} allows the device to be used directly by a device driver running in a \gls{guest}, requiring that compute tasks run in \glspl{vm} is a constraint that may add additional system load, as \gls{cpu} cycles are spent on \lgls{host}{hosting} the virtualized environment.
%
Another mechanism for using \glspl{ntb} while remaining agnostic about the address space in remote systems is therefore needed.
%
The physical location of a resource, as well as the address space layout in the machine it is installed in, must be abstracted away without requiring \glspl{vm} (but sharing resources to \glspl{vm} should \emph{also} be supported).



Nevertheless, this kind of abstraction gives rise to another challenge. 
%
A device driver that is unaware that a device is remote may assume that the entire local address space can be reached by the device.
%
It is generally not possible to predict in advance which memory addresses a device driver may use, yet \gls{ntb} mappings must be in place before the driver initiates operations, such as \gls{dma} transfers.
%
Deferring the action of setting up these mappings until the device driver initiates \gls{dma}, or some other time when the specific addresses of \gls{dma} buffers may be known, is not realistic;
%
synchronizing with the remote system at this time will introduce communication overhead in the performance-critical path.
%
The naive solution is to map the entire system memory of all other machines in the cluster, but this workaround does not scale very well as the memory requirements will then increase for every machine added to the cluster.
%
A way to prepare the necessary memory-mappings through the \gls{ntb}, without adding additional communication latency, is required.




\section{Problem statement}\label{sec:problem}
Utilizing \gls{pcie} \glspl{ntb} to share resources among machines in a \gls{pcie}-networked cluster requires a mechanism for abstracting away the physical location of a resource, including the address space of the computer system it is installed in. 
%
More specifically, as it is desirable to avoid modifications to existing device drivers and application software, such a mechanism must also be able to present resources to the system as if they were locally installed.
%
This mechanism must also allow remote resources to be used with the same performance expected for native \gls{pcie},~i.e., with the same performance as if they were attached to the local \gls{pcie} bus.
%
Hence, the goal of this dissertation is to develop a solution for sharing and distributing \gls{io}~resources (devices and memory) in a way that eliminates the distinction between local and remote access of a resource. 
%
The challenges of this goal are addressed under the following research question: 
\begin{restatable}{question}{researchquestion}\label{question}
    Can \glspl{ntb} be leveraged to allow the internal memory and devices of individual computers in a \gls{pcie}-networked cluster to be shared with and used by remote machines in the cluster, as if these resources were local to the remote machines?
\end{restatable}
%
In particular, this research~question can be broken down into the following list of requirements that a solution must satisfy:


\begin{objective}\label{obj:distributed}
    Ubiquitous sharing in the cluster should be supported, allowing any machine to contribute any of its internal \gls{pcie} devices, and allowing any machine to be able to use shared devices, even contributing and using devices at the same time.
\end{objective}
The main motivation for our goal of building a system for \gls{io} resource sharing is to make it easier to scale out and use more resources than there are available in a single computer. 
If any standard \gls{pcie} device inside any machine could be shared with other machines, the \gls{io} resource utilization in the cluster could be greatly increased.
Additionally, by avoiding dedicated servers and allowing all computers in the cluster to participate in the sharing, contributing their own resources and using resources shared by others, we would effectively enable a distributed, peer-to-peer sharing model. 
This objective sets our goal apart from existing \gls{pcie}-based solutions, as these require a central server or devices that are directly attached to a \gls{pcie} switch.



\begin{objective}\label{obj:transparent}
    The fact that resources may be remote should be functionally transparent, allowing systems to use remote resources in the same way as if they were local, without requiring any modifications to device hardware, device drivers, host \gls{os}, or application software.
\end{objective}
If the solution could make remote devices behave as if they were locally installed, presenting resources to the system on a level ``underneath'' the \gls{os}, it would become possible to distribute devices to \emph{physical} hosts as well, and not only \glspl{vm}. 
In other words, remote resources should appear as if they were part of the local \gls{pcie} device tree, and application software could make use of remote devices using native interfaces in the same way it would use local devices.
%
Furthermore, by avoiding application- or device-specific \gls{middleware}, and instead memory-mapping remote system and device memory directly, existing device drivers and even the host \gls{os} itself would be able to interact with remote resources natively.
Avoiding any special adaptions to software would make scaling out significantly easier than what is currently possible with existing \gls{middleware}-based solutions for distributed \gls{io}, particularly those based on \gls{rdma}.



\begin{objective}\label{obj:performance}
    The fact that resources may be remote should be transparent with regard to performance, remote resources should be used with native \gls{pcie} performance, and as close to local access as possible.
\end{objective}
Moving data to remote units over the network introduces large performance overhead compared to accessing local resources. 
In order to further blur the hard separation between ``remote'' and ``local'', remote resources should not only behave functionally as if they were locally installed in the system using them, but also have comparable performance.
To achieve this, any communication overhead and intermediate data copying in the critical path must be completely avoided, a requirement that rules out (most) traditional methods of sharing resources over a network. 
Remote resources should be accessed directly over \emph{native} \gls{pcie}, which would improve the overall \gls{io} performance in the cluster.



\begin{objective}\label{obj:dynamic}
    Shared resources should be distributed \emph{dynamically}, and direct access to device memory and system memory should be configured in run-time, also between multiple devices residing in different hosts.
\end{objective}
As stated in \cref{obj:transparent}, the solution should work for physical hosts, and not only \glspl{vm}. Therefore, it must be possible to assign and reassign resources while all machines in the cluster are running, without requiring rebooting hosts or changing settings in the \gls{bios}.
For devices, this introduces the requirement that the \gls{os} supports hot-adding devices to the system (which most modern \gls{os} implementations do).
Not only would this would allow systems to dynamically scale up or down their \gls{io} resources based on immediate workload requirements, but devices could be more efficiently partitioned between machines in the cluster, increasing the overall resource utilization. 
Furthermore, the solution should also be able to automatically discover resource location, without requiring that the user knows anything about the underlying \gls{pcie} network topology, and dynamically set up memory mappings between devices, \glspl{cpu}, and memory resources. An example would be enabling \gls{pcie} \gls{p2p} between two or more \gls{dma}-capable devices that are physically installed in different machines.

    
\begin{objective}\label{obj:disaggregation}
    \Gls{disaggregation} of system memory, device memory, and device \emph{functionality} should be supported, and the solution should be able to distribute component parts to different hosts, as well as provide software facilities for resources that do not support \gls{disaggregation} in hardware.
\end{objective}
Because most device drivers are written in a way that assumes exclusive control over a device, some devices implement virtualization support in hardware,~i.e., \gls{sriov}, that makes them appear to a system as having multiple \glspl{vf}. 
%
The solution should be able to \lgls{disaggregation}{disaggregate} such \gls{sriov}-capable devices, and distribute their \glspl{vf} to different machines, allowing multiple computers to use the same device simultaneously.
%
However, since not all devices implement \gls{sriov}, the solution should also provide a device driver \gls{api} that will make it possible to \lgls{disaggregation}{disaggregate} memory and device resources in software.
%
In addition to the native sharing capabilities described in \crefrange{obj:distributed}{obj:dynamic}, this \gls{api} 
would provide facilities for memory-mapping device registers as well as mapping shared memory segments for a \gls{dma}-capable device.
%
Effectively, this would bring shared-memory concepts to device driver implementations, allowing device operation and device resources to become part of the same global address space as distributed cluster applications.
%
This would allow multiple machines to simultaneously share the same, non-\gls{sriov} device, as well as making it possible to combine traditional \gls{io} with \gls{pcie} cluster capabilities such as zero-copy data transfer and multicasting.
%
Moreover, the \gls{api} should be designed so that a driver implementation does not need to consider the system-local address space of the computer system where a device is installed, thus alleviating the complexity of programming device drivers for remote devices using \glspl{ntb}.



\begin{objective}\label{obj:experiments}
    To prove real-world deployment capabilities, the solution should be tested on realistic and relevant workloads and benchmarks.
\end{objective}
To confirm that \gls{io} resources can be distributed to, and shared with, remote machines, a comprehensive performance evaluation covering all components of the implementation is needed.
As the solution should provide \emph{native} \gls{pcie} performance (\cref{obj:performance}), all parts should be thoroughly tested with latency and throughput in mind, in order to reveal any potential performance bottlenecks.
Standardized test suites should be used as far as possible, to prove that application software really can be unmodified (\cref{obj:transparent}).
%
Moreover, to demonstrate the completeness of the solution, the evaluation should also include workloads relying on different \gls{pcie} network topologies and include several types of devices, such as \glspl{nvme}, \glspl{gpu}, and \glspl{nic}.
%
Finally, a prototype device driver using the device driver \gls{api} (\cref{obj:disaggregation}) should be developed and evaluated. This driver should demonstrate that it is possible to \lgls{disaggregation}{disaggregate} a non-\gls{sriov} device in software, and shared with multiple machines simultaneously. 
%
The driver should also demonstrate how it can rely on memory \gls{disaggregation} and shared memory capabilities to implement data path optimizations.



\section{Scope and limitations}\label{sec:scope}
% limitation: systems must support pcie p2p (or use switches that do)
%limitation: systems must support hot-plugging
%not disaggregated os, not general networks, not 50,000 nodes etc
% outside scope: orchestration, algorithms for sharing, fairness, etc
% scope
% both physical hosts and virtual hosts
% also shared memory applications
% distributed, unlike existing solutions, true peer-to-peer sharing
% commodity servers, multi platform amd, intel, arm, no supercomputing

% performance, how much performance overhead in using remote devices?

% limitations
% safety, security (iommu, encryption)
% must use ntbs (dolphin ntbs), only sisci
% not a finished product (no orchistration sw [yet])
% for linux only?
%Additionally, different Linux distributions, kernel versions, and application software versions should be used, to demonstrate that the solution is not limited to a specific Linux version.

\gls{sisci}
TODO

\section{Research methodology}\label{sec:methodology}
Choosing an appropriate research methodology for problems in computer~science is challenging.
%
Many methodologies come with their own set of considerations and potential pitfalls~\cite{McGrath1981}.
%
Finding a methodology is not made easier by the fact that computer~scientists themselves do not seem to agree on the age-old philosophical question of whether computer~science should be considered applied mathematics, engineering, or a science~\cite{Denning2005}.


According to Eden~\cite{Eden2007}, computer~science as a discipline can broadly be divided into three different research paradigms:
\begin{itemize}
    \item The rationalist paradigm, which defines computer~science as a branch of mathematics. 
        The paradigm seeks certain, a~priori knowledge of systems or processes through means of deductive reasoning.
    \item The technocratic paradigm, which defines computer~science as an engineering discipline.
        The paradigm seeks probable, a~posteriori knowledge about systems or processes through implementation (or prototyping) and empirical validation in the form of testing.
    \item The scientific paradigm, which defines computer~science as a natural (empirical) science.
        The paradigm seeks both a~priori and a~posteriori knowledge about systems or processes by combining formal deduction and scientific experimentation.
\end{itemize}
Note that the technocratic concept of a ``test'' differs from scientific experiments, in that the purpose of the former is to establish to which extent a set of requirements are met, whereas the latter is designed to corroborate or refute a hypothesis.
%%
If a test fails (to meet a requirement), the implementation must be revised. 
%
If an scientific experiment ``fails'', it is the theory (or understanding) that must be revised instead.


A similar classification of paradigms is given by the ACM Task Force on the Core of Computer Science~\cite{Comer1989}.\footnote{ACM uses the names ``theory'', ``design'', and ``abstraction'' for these paradigms instead.}
%
They additionally note that the three paradigms are intrinsically intertwined, as computer~science is both deeply rooted in mathematics and has its own theory, experimental method, and engineering---in contrast to most physical sciences, where the engineering disciplines that apply their findings are considered separate.
%
The paradigms are therefore equally fundamental to computer~science.



The subject matter of this dissertation touches on several sub-areas of computer~science, including computer and hardware architecture, distributed computing, \gls{os} fundamentals, and communication networks.
%
While all three paradigms can be applied to these sub-areas~\cite{Comer1989}, it is the technocratic paradigm which lends itself best to answering the overall problem~statement of this dissertation~(\cref{sec:problem}).
%
Neither the rationalist nor the scientific paradigm are particularly well-suited.
%
It is unrealistic to make a model through a~priori knowledge alone, due to the complexity of the many different hardware and software components of real-world computer systems.
%
Similarly, the process of gathering data with the goal of understanding the behavior of an indeterministic system, in order to create a statistical model and make predictions about it, seems equally unfit in this context.



As many solutions for distributed \gls{io} already exist, the motivation for this dissertation is not simply to make it easier for machines to share resources efficiently, but to do so by using a new approach altogether:
%
we attempt to unify traditional device \gls{io} with distributed, shared-memory computing by utilizing the inherent memory mapping capabilities of \glspl{ntb}.
%
As such, it is desirable to realize this approach by \textbf{building a working prototype}, in order to be able to explore potential opportunities and weaknesses along the way.
%
Therefore, we follow the technocratic paradigm by iteratively designing, implementing, and testing a solution according to the requirements and specifications given in \cref{sec:problem}:
%
\begin{itemize}
    \item 
        A mechanism must be developed to make remote resources appear and behave exactly as if they were part of the local \gls{pcie} tree.
        %
        In order to not require any special adaptions of existing hardware, device drivers, or application software, this mechanism must be completely transparent.
        %
        This mechanism must also be thoroughly tested using a wide range of functionality tests, for many different types of devices and device driver implementations.
        %
        As we make a point of using unmodified hardware and software, commodity devices and widely available software, such as standard benchmarking tools and well-known applications, should be used in the validation of our solution.
        %
        A complete functionality evaluation should also include a variety of different cluster network topologies and machine configurations, including \glspl{vm}.

    \item 
        A wide range of latency and throughput benchmarks should be used to measure the performance of the critical \gls{io} data path.
        %
        Since the solution allows machines to use the internal devices and memory in other (remote) machines as if these resources were locally installed, it is possible to rely on comparison testing.
        %
        Tests looking at individual \gls{io} operations, such as reading/writing to an \gls{nvme} or copying memory out of \gls{gpu} memory, can first be performed using local resources (without our solution) to establish a performance baseline.
        %
        Then, the tests can repeated for remote resources using our solution, allowing us to compare the results and subsequently identify which component of the solution that needs to be improved for the next iteration.
        %
        Note that the different iterations and gradual improvements of our solution can be seen in \crefrange{paper:nossdav}{paper:tocs}.

    \item 
        Our implementation process should also identify and explore new possibilities that are enabled by the solution.
        %
        The strength of our shared-memory approach is best highlighted by demonstrating capabilities that other resource sharing solutions lack.
        %
        For example, as CUDA unified memory~\cite{url:unified-memory} can be supported by our solution, even for \glspl{gpu} that reside in \emph{different machines}, we should perform experiments with direct memory-to-memory transfers across the cluster network.
        %
        Other possibilities, such as exploiting memory \gls{disaggregation} to implement memory locality optimizations or using \gls{pcie} \gls{multicasting} to replicate data across several machines in a single operation, should also be explored.
        %
        We should not only investigate how different cluster network topologies affect the data path, but we should also prove the flexibility of our solution and demonstrate several of the sharing scenarios that are made possible.

    \item
        A realistic and \gls{io}-intensive computing task, for example involving machine~learning, should be used to put the solution under heavy load.
        %
        By running a real-world application using several \gls{io} resources, we can observe the accumulated effects of any performance overhead caused by the implementation that are not visible on their own.
        %
        Moreover, this kind of stress testing also proves that the solution is stable and gives reliable performance for repeated runs, and that it does not have any unintentional side-effects that affect systems over time.
        %
        Finally, showing that the solution works for a realistic workload has the additional purpose of proving that scaling real-world applications is possible.

\end{itemize}
%
For the sake of convenience, functionality testing and validation process is implicit in the performance experiment results presented in this dissertation.
%
It should also be noted that while the presented experiments primarily use Intel~Xeon \glspl{cpu} and \glspl{nvme}, \glspl{nic}, and Nvidia \glspl{gpu}, additional \gls{cpu} architectures and devices, like \glspl{fpga} and sound cards, were used under development of the solution.


\section{Contributions}
This dissertation contributes to the topic of \gls{io} facilitation and resource sharing in distributed computing systems, and has been presented in five peer-reviewed venues: two conference workshop publications, one short-length demonstration paper, and two journal articles.
These publications are included as \crefrange{paper:nossdav}{paper:tocs} and contain the bulk of the implementation details, particularly \cref{paper:tocs}, which presents the entire solution as a whole.

We have developed a system called \emph{SmartIO} for sharing resources and distributing devices in a heterogeneous, \gls{pcie}-networked cluster.
%
In particular, the main contributions of this dissertation are listed as follows:
\begin{itemize}
    \item Testing and evaluation of the \emph{Device~Lending} mechanism for \textbf{distributing \gls{pcie} devices to remote systems} (see \cref{paper:nossdav} and \cref{paper:mmsys}): 
        %
        using Device~Lending, any standard \gls{pcie} device, such as \glspl{nvme}, \glspl{gpu}, \glspl{nic}, and \glspl{fpga}, may be assigned to a remote system. 
        %
        The device appears to the remote system as if it has been dynamically hot-added to the system, allowing existing device drivers to use the device without requiring any modifications to software.

    \item Implementation of a \textbf{new mechanism for distributing devices to \glspl{vm}} running on any host machine in the cluster (see \cref{paper:srmpds} and \cref{paper:cc}): 
        %
        we have developed an extension to the \gls{kvm} based on the \emph{\gls{mdev}} interface, enabling direct access to remote physical hardware devices for \gls{vm} guests and setting up memory mappings for the devices. 
        %
        This \gls{mdev} implementation includes a method for dynamically discovering guest-physical memory layout. Using this \gls{mdev} extension, local and remote devices can be \lgls{passthrough}{``passed through''} to \glspl{vm} and used with bare-metal performance.
	
    \item Improvement of the Device~Lending and \gls{kvm}/\gls{mdev} mechanisms by implementing \textbf{support for multiple devices} and \textbf{supporting devices in different physical machines} (see \cref{paper:srmpds} and \cref{paper:cc}):
        %
        a method for resolving device memory addresses and setting up memory mappings, in a way that is transparent to both the devices and the device drivers, is implemented. 
        %
        This enables direct data transfers between multiple devices without violating the principle of making devices appear local to the system(s) using them.

    \item Extension of the \gls{sisci} shared-memory \gls{api} with new, \textbf{device-oriented programming semantics} for writing device drivers as shared-memory applications (see \cref{paper:tocs}):
    %
	this \gls{api} extension makes it possible to \lgls{disaggregation}{disaggregate} devices and device memory in software, similarly to \gls{rdma} \gls{disaggregation} solutions.
    %
	Unlike \gls{rdma} solutions, however, remote resources can be memory-mapped directly into the virtual address space of a software process.
	%
	Through our \gls{api} extension, device driver implementations may take full advantage of \gls{pcie} shared memory capabilities, such as remote memory access and multicasting, without requiring awareness of the underlying \gls{pcie} topology and the different address domains of remote systems.
	%
	This makes it easier to optimize data flow through the \gls{pcie} network, as software no longer needs to be written with accessing remote resources in mind, but can be implemented as if resources are local.

    \item Development of a \textbf{new prototype \gls{nvme} device driver}\footnote{The prototype \gls{nvme} device driver is open source and can be found at \mbox{\url{https://github.com/enfiskutensykkel/ssd-gpu-dma}}} using our device-oriented \gls{api} extension (see \cref{paper:tocs}):
    although the Device~Lending mechanism and \gls{mdev} extension make it possible to use existing device drivers, most device drivers are written in a way that assumes exclusive control over the device. Therefore, a distributed device (function) may only be used by a single user at the time.
        %
    To demonstrate software-enabled \gls{disaggregation}, we have implemented a \emph{distributed} \gls{nvme} driver. 
    %
    As a proof of concept, we show that a single NVMe device can be shared and operated by multiple cluster machines simultaneously, without requiring \gls{sriov}.
	This driver also demonstrates how multiple sharing aspects of SmartIO may be combined, 
	by \lgls{disaggregation}{disaggregating} remote GPU memory and enabling memory access optimizations.

    \item A \textbf{comprehensive performance evaluation} covering all parts of SmartIO and the implementation of performance optimizations (see \cref{paper:mmsys}, \cref{paper:cc}, and \cref{paper:tocs}):
        %
        with the goal of not incurring any performance overhead beyond that of native \gls{pcie}, the performance of using remote resources with SmartIO is comparable to that of local access (in terms of latency and throughput).
        %
        To prove that SmartIO is a viable and efficient solution for \gls{io} resource sharing also for realistic scenarios, two different image classification workloads relying on multiple \glspl{gpu} and \gls{nvme} storage have also been tested.
	
\end{itemize}
%
Finally, it should be noted that the research of this dissertation has had impact on real systems, as several components of SmartIO have already been incorporated into the product line of Dolphin Interconnect Solutions, and others are currently being adapted for real-world deployment.\footnote{{\url{https://www.dolphinics.com/solutions/pcie_smart_io.html}}}


\section{Outline}
This dissertation describes the SmartIO system for efficient sharing of resources between \gls{pcie}-networked computers.
%
%something something iterations
%The different iterations and gradual improvements of our solution in order to better satisfy the requirements are seen in \crefrange{paper:nossdav}{paper:tocs}.
%The final solution is outlined in \cref{chapter:smartio}, with a more detailed description given in \cref{paper:tocs}.
The rest of this thesis is organized as follows:
\begin{description}
    \item[\cref{chapter:smartio}]
        presents the overall ideas and challenges for SmartIO. 
        We give a high-level overview of the implementation and 
        also related work

    \item[\cref{chapter:conclusion}]

    \item[\cref{paper:nossdav}]

    \item[\cref{paper:mmsys}]

    \item[\cref{paper:srmpds}]

    \item[\cref{paper:cc}]

    \item[\cref{paper:tocs}]
\end{description}

