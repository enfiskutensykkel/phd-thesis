\chapter{SmartIO}\label{chapter:smartio}
SmartIO is a solution for allowing the local resources of a machine,~i.e., memory and devices, to be shared with and used by remote machines, over standard \gls{pcie}.
%
SmartIO works for \emph{all} standard \gls{pcie} devices and their Linux device drivers, no special adaptation is needed in either hardware or software to make this sharing possible.
%
Whether devices are actually local or remote becomes irrelevant to the user, as SmartIO eliminates this distinction, with regard to both functionality and performance.
%
Furthermore, individual device functions of multi-function devices may be distributed to different machines in the network, or to the same machine should it require multiple resources.
%
It is even possible to \lgls{disaggregation}{disaggregate} a single device~(function) in software, and distribute it to multiple machines, should this be required by application software.
%
In other words, SmartIO is a solution for scaling out and using more hardware resources than there are available in a single machine.


In this chapter, we provide an overview of the SmartIO solution.
%
The complete system is described in detail in \cref{paper:tocs}.
%
\Crefrange{paper:nossdav}{paper:cc} show the evolution towards this complete system, including different iterations of individual components and parts as well as gradual performance improvements.


\section{Underlying idea}\label{sec:idea}
The defining feature of \gls{pcie} is that devices are mapped into the same address space as the \gls{cpu} and \gls{ram}.
%
This allows a \gls{cpu} to read from and write to device memory in the same manner it would access \gls{ram}.
%
Likewise, devices capable of \gls{dma} may read from and write to \gls{ram} directly.
%
\Gls{pcie} also uses \gls{msi}, allowing devices to raise interrupts by writing to an address reserved by the \gls{cpu} instead of requiring dedicated interrupt lines.


This mapping occurs when a system enumerates the \gls{pcie} bus and accesses the configuration space of each device.
%
The configuration space contains a description of the capabilities of the device, such as its memory regions.
%
The system will reserve a memory address range for each of these device memory regions.
%
By writing the start address of these regions to the device's \glspl{bar}, the device is made aware of the address space mapping.
%
Therefore, the term ``\gls{bar}'' is used synonymously for device memory regions.

%
Devices are made aware of 
The system reserves a memory address range for each of these memory regions

%introduce later: A device may even access other devices, as they too are mapped into the same address space.
However, as this mapping occurs when a \gls{cpu} enumerates the \gls{pcie} bus

By using an \gls{ntb} implementation, it is possible to connect computer systems with independent address domains together over \gls{pcie}.
%
Although not formally standardized
\begin{figure}
    \centering
    \includegraphics{ntb-example}
    \caption{test}
    \label{fig:ntb-example}
\end{figure}


\Glspl{ntb} are the most common way 

\gls{pcie}

\Glspl{ntb} are a special type ofA

\paperref{tocs}{pcie-addr}
\begin{itemize}
    \item a combined (and \textbf{shortened}!!) version of PCIe overview + NTB + mapping bars and DMA windows from previous papers
    \item simple ntb figure for mapping memory on remote node
    \item the famous device lending figure (but in a ``is this possible?'' kind of way)
    \item the general idea: can stuff (memory + bars + interrupts) be mapped through the NTB in a way that is transparent for OS?
\end{itemize}


%Unlike existing solutions for distributed \gls{io}, SmartIO seamlessly combines traditional \gls{io} with distributed shared-memory functionality, and is, therefore, able to provide sharing capabilities at multiple abstraction levels.
%Devices may be distributed to physical hosts and to \glspl{vm} alike, and SmartIO also provides facilities for \glslink{disaggregation}{disaggregating} devices and memory resources in software.


%However, as resources are accessed over native \gls{pcie}, they can be shared and used by remote machines without introducing a performance penalty.
%%
%Also unlike existing \gls{pcie}-based solutions, SmartIO is fully distributed and avoids dedicated servers.
%All machines in the cluster can contribute their own local resources and access remote resources, even at the same time.
%%
%Finally, by using \gls{pcie} shared memory techniques, SmartIO is able to abstract away the physical location of devices and memory resources. 
%Memory addresses are translated between different address domains by SmartIO in a manner that is transparent to application software, device drivers, and even the \gls{os}.
%This makes it possible to provide optimizations based on resource locality and minimizing data movement, without requiring the user to be aware of the underlying PCIe topology.
%\section{General idea}\label{sec:idea}
\section{Implementation}\label{sec:impl}
High level explanation of borrowers and lenders here.
Software architecture figure (showing the different components here)

\subsection{Device Lending}
\begin{itemize}
    \item explain shadow device
    \item intercept configuration cycles
    \item hooking DMA api for setting up DMA windows
    \item IOMMU discussion
\end{itemize}

\subsection{MDEV}
\begin{itemize}
    \item \textbf{very} brief: what is passthrough/VFIO
    \item what MDEV gives us
    \item how to map VM memory for device
    \item relaying interrupts
\end{itemize}

\subsection{API extension}
\begin{itemize}
    \item why do we need API? (disaggregation in software + shared memory networking)
    \item short explanation of SISCI
    \item what functions did we add to SISCI?
\end{itemize}

\subsection{Support for multiple lenders}
\begin{itemize}
    \item peer-to-peer device lending
    \item peer-to-peer vms
\end{itemize}


\subsection{NVMe driver}\label{sec:nvme}
\begin{itemize}
    \item why a prototype driver? demonstrate disaggregation in software
    \item why nvme? easy to parallelize because of queues
    \item driver implementation and queue sharing
    \item gpudirect + co-operation with device lending
\end{itemize}

%\section{Workload}\label{sec:eval}

\section{Performance measurements}\label{sec:eval}
Some selected performance graphs here, demonstrating zero-overhead :)

\section{Related work}\label{sec:rw}
should rw go in discussion and conclusion instead?
\begin{itemize}
    \item rdma stuff
    \item fabric partitioning: mr-iov, broadcom/microsemi chips with partitioning
    \item ladon (and other ntb stuff)
    \item disaggregation
\end{itemize}

% RW first??
%An example of this is rCUDA, where multiple clients may run jobs on the same \gls{gpu} by relying ~\cite{Duato2010}
%\Gls{rdma}-based \gls{disaggregation} can even allow a single resource to be shared by several machines in the cluster at the same time. 
%
%However, since 
