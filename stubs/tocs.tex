\chapter{SmartIO: Zero-overhead Device Sharing through PCIe Networking}
\label{paper:tocs}
\paperthumb

\begin{description}
	\item[Authors:]
		\textbf{Jonas Markussen}, Lars Bj{\o}rlykke Kristiansen, P{\aa}l Halvorsen,
		Halvor Kielland-Gyrud, H{\aa}kon Kvale Stensland, Carsten Griwodz.

	\item[Abstract:]
		The large variety of compute-heavy and data-driven applications accelerate the need for a distributed
		\lgls{io}{I/O} solution that enables cost-effective scaling of resources between networked hosts. For example,
		in a cluster system, different machines may have various devices available at different times, 
		but moving workloads to remote units over the network is often costly and introduce 
		large overheads compared to accessing local resources. 
		%
        To facilitate \lgls{io}{I/O} \gls{disaggregation} and device sharing among hosts connected using \lgls{pcie}{Peripheral Component Interconnect Express (PCIe)} 
		\lgls{ntb}{non-transparent bridges}, we present SmartIO. \lgls{nvme}{NVMes}, \lgls{gpu}{GPUs}, network adapters, 
		or any other standard PCIe device may be borrowed and accessed directly, as if they were local to the remote machines.
		%
		We provide capabilities beyond existing \gls{disaggregation} solutions 
		by combining traditional \lgls{io}{I/O} with distributed shared-memory functionality, allowing devices 
		to become part of the same global address space as cluster applications.
		Software is entirely removed from the data path, and simultaneous sharing of a device among 
		application processes running on remote hosts is enabled.
		%
		Our experimental results show that \lgls{io}{I/O} devices can be shared with remote hosts,
		achieving native \lgls{pcie}{PCIe} performance.
		%
		Thus, compared to existing device distribution mechanisms, SmartIO provides more efficient, low-cost resource
		sharing, increasing the overall system performance.

	\item[Candidate's contributions:]
		The ideas for the device-oriented extension to the \gls{sisci}~\gls{api} grew out from Markussen's experiences with implementing MDEV for \cref{paper:srmpds} and \cref{paper:cc}.
		%
		Markussen contributed with several new ideas for the design of this \gls{api}, and implemented these.
		%
		He collaborated on the effort of combining these ideas with previous work into the complete SmartIO system. 
		%
		Furthermore, Markussen came up with the idea for, designed, and implemented the prototype distributed \gls{nvme} driver using this \gls{api} extension, 
		including the queue offloading idea and support for running the driver on \glspl{gpu}.
		%
		Limitations in the initial \gls{api} design were uncovered during this process, and Markussen made subsequent improvements to both design and implementation of the \gls{api} throughout the development of the driver.
		%
		Additionally, he designed and implemented several workloads for this \gls{nvme} driver using \glspl{gpu} and Device~Lending
		in order to demonstrate the novelty and completeness of the SmartIO solution, as well as the performance benefits.
		%
		He conducted a thorough and exhaustive performance analysis of all components of the SmartIO system,
		as well as evaluating the entire system, and investigated and implemented solutions for eliminating performance overheads in the system.
		%
		Finally, Markussen wrote most of the text for the paper, and also wrote the necessary tools and benchmarking programs for the evaluation,
		including the \gls{fio} integration for the \gls{nvme} driver and implementing \gls{gpu} and \gls{nvme} test programs.
		

	\item[Published in:]
		\emph{Transactions on Computer Systems}. ACM.
		Published online~June~2021,
		issue~date~July~2021, 
		volume~38, issue~1-2, article~2, pp.~2:1--2:78.

	\item[DOI:] \href{https://doi.org/10.1145/3462545}{10.1145/3462545}

	\item[Contributed to:]
        All objectives (\cref{obj:distributed,obj:transparent,obj:performance,obj:dynamic,obj:disaggregation,obj:experiments}).

\end{description}

\includepaper[numbers=low][last]{tocs}{
	2, section, 1, Introduction, tocs:intro,
	5, section, 1, System overview, tocs:overview,
	6, subsection, 2, Motivation and challenges, tocs:overview-motivation,
	7, subsection, 2, Overall design, tocs:overview-design,
	9, section, 1, PCIe-interconnected clusters, tocs:pcie,
	9, subsection, 2, PCIe endpoints, tocs:pcie-ep,
	10, subsection, 2, Address-based routing, tocs:pcie-addr,
	11, subsection, 2, Non-transparent bridging, tocs:pcie-ntb,
	13, section, 1, Device Lending, tocs:lending,
	13, subsection, 2, Shadow device, tocs:lending-vdev,
	14, subsection, 2, Intercepting configuration cycles, tocs:lending-cfgspace,
	14, subsection, 2, DMA window, tocs:lending-dma,
	15, subsection, 2, Shortest path routing, tocs:lending-p2p,
	16, section, 1, VM pass-through using MDEV, tocs:mdev,
	17, subsection, 2, Mediated devices, tocs:mdev-vfio,
	18, subsection, 2, Mapping VM memory for device, tocs:mdev-dma,
	20, subsection, 2, Peer-to-peer between devices, tocs:mdev-p2p,
	20, subsection, 2, Relaying interrupts, tocs:mdev-intr,
	21, subsection, 2, VM migration, tocs:mdev-migration,
	21, section, 1, Distrubted NVMe driver, tocs:nvme,
	22, subsection, 2, Device driver API, tocs:api,
	23, subsection, 2, Driver implementation, tocs:nvme-impl,
	25, subsection, 2, Multipath failover, tocs:nvme-failover,
	26, subsection, 2, GPU support, tocs:nvme-gpu,
	29, subsection, 2, Multicast, tocs:nvme-mcast,
	29, section, 1, Performance evaluation, tocs:eval,
	31, subsection, 2, Device Lending, tocs:eval-lending,
	31, subsubsection, 3, Latency tests, tocs:eval-lending-lat,
	33, subsubsection, 3, Throughput tests, tocs:eval-lending-bw,
	35, subsubsection, 3, Longer PCIe paths, tocs:eval-lending-path,
	37, subsubsection, 3, Peer-to-peer: local vs. remote, tocs:eval-lending-p2p-1L,
	39, subsubsection, 3, Peer-to-peer: multiple lenders, tocs:eval-lending-p2p-2L,
	41, subsubsection, 3, Sharing SR-IOV devices, tocs:eval-sriov,
	45, subsection, 2, Scaling heavy workloads, tocs:eval-ml,
	46, subsection, 2, VM pass-through with MDEV, tocs:eval-mdev,
	47, subsubsection, 3, IOMMU performance penalty, tocs:eval-iommu,
	48, subsubsection, 3, Pass-through comparison, tocs:eval-mdev-vfio,
	50, subsection, 2, Distributed NVMe driver evaluation, tocs:eval-nvme,
	52, subsubsection, 3, Optimizing data access patterns, tocs:eval-nvme-sq,
	54, subsubsection, 3, Sharing a single-function NVMe device, tocs:eval-nvme-sharing,
	56, subsubsection, 3, NVMe-oF RDMA comparison, tocs:eval-nvmeof,
	60, section, 1, Discussion, tocs:disc,
	60, subsection, 2, Security, tocs:disc-security,
	61, subsection, 2, Supported OSes, tocs:disc-os,
	62, subsection, 2, Supported CPU architectures, tocs:disc-cpu,
	62, subsection, 2, Supported devices, tocs:disc-devs,
	63, subsection, 2, Alternative NTB implementations, tocs:disc-ntbs,
	63, subsection, 2, Scalability, tocs:disc-scalability,
	65, subsection, 2, Disaggregated and composable infrastructure, tocs:disc-composable,
	66, section, 1, Related work, tocs:rw,
	66, subsection, 2, PCIe fabric partitioning, tocs:rw-partitioning,
	67, subsection, 2, NTB-based solutions, tocs:rw-ntb,
	68, subsection, 2, Distributed I/O using RDMA, tocs:rw-rdma,
	70, subsection, 2, NVMe queue distribution, tocs:rw-nvme,
	70, subsection, 2, Memory disaggregation, tocs:rw-disaggr,
	72, section, 1, Conclusion, tocs:concl
}
